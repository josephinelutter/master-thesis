{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "044a7955-4349-48f7-8bec-7b62b8892ad3",
   "metadata": {},
   "source": [
    "## Notebook 3 - All locations\n",
    "### **Predicting Dengue Fever Incidence and Disease Dynamics under Climate Change in Southeast Asia**\n",
    "### Master's thesis by Josephine Lutter, supervised by Professor Roberto Henriques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22d6e28-1e4b-4e2c-bf1b-9d26c12142f3",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<ul>\n",
    "  <li><a href=\"#1.-Import\">1. Import</a></li>\n",
    "  <li><a href=\"#2.-Data-Exploration\">2. Data Exploration</a>\n",
    "    <ul>\n",
    "      <li><a href=\"#a.-df_3\">a. df_3</a></li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li><a href=\"#3.-Data-Preparation\">3. Data Preparation</a>\n",
    "    <ul>\n",
    "      <li><a href=\"#a.-Feature-Removal\">a. Feature Removal</a></li>\n",
    "      <li><a href=\"#b.-Outlier-Detection\">b. Outlier Detection</a></li>\n",
    "      <li><a href=\"#c.-Outlier-Removal\">c. Outlier Removal</a></li>\n",
    "      <li><a href=\"#d.-Feature-Creation\">d. Feature Creation</a></li>\n",
    "      <li><a href=\"#e.-Data-Encoding\">e. Data Encoding</a></li>\n",
    "      <li><a href=\"#f.-Data-Partition\">f. Data Partition</a></li>\n",
    "      <li><a href=\"#g.-Data-Normalization\">g. Data Normalization</a></li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li><a href=\"#4.-Feature-Selection\">4. Feature Selection</a></li>\n",
    "  <li><a href=\"#5.-Predictive-Modeling\">5. Predictive Modeling</a>\n",
    "    <ul>\n",
    "      <li><a href=\"#5.1-Hyperparameter-Tuning\">5.1 Hyperparameter Tuning</a>\n",
    "        <ul>\n",
    "          <li><a href=\"#a.-Scale-invariant-models\">a. Scale-invariant models</a></li>\n",
    "          <li><a href=\"#b.-Scale-variant-models\">b. Scale-variant models</a></li>\n",
    "        </ul>\n",
    "      </li>\n",
    "      <li><a href=\"#5.2-Cross-Validation-and-Model-Evaluation-on-Training-Set\">5.2 Cross-Validation and Model Evaluation on Training Set</a>\n",
    "        <ul>\n",
    "          <li><a href=\"#a.-Scale-invariant-models\">a. Scale-invariant models</a></li>\n",
    "          <li><a href=\"#b.-Scale-variant-models\">b. Scale-variant models</a></li>\n",
    "        </ul>\n",
    "      </li>\n",
    "      <li><a href=\"#5.3-Final-Training-and-Prediction-on-Test-Set\">5.3 Final Training and Prediction on Test Set</a>\n",
    "        <ul>\n",
    "          <li><a href=\"#a.-Scale-invariant-models\">a. Scale-invariant models</a></li>\n",
    "          <li><a href=\"#b.-Scale-variant-models\">b. Scale-variant models</a></li>\n",
    "        </ul>\n",
    "      </li>\n",
    "    </ul>\n",
    "  </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ddd05-a48c-4663-8571-486c01cde612",
   "metadata": {},
   "source": [
    "## 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9cabe8-3b84-481f-af39-ba54d03ceb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for Data Exploration\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Imports for Data Preprocessing and Engineering\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.feature_selection import RFE, SequentialFeatureSelector, SelectFromModel\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LassoCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Imports for Modeling\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbc47e7-3a82-4d49-b4a9-7335975a7413",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgb.__version__)\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a79034-eef5-4513-a02d-8efb7d3285a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration of Excel files that have been pre-structured using Mircosoft Excel to guarantee consistent structure\n",
    "\n",
    "file = \"/Users/Fine/Documents/Master Business Analytics/Thesis/Research Data/Final Data/Combined/All locations combined.xlsx\"\n",
    "\n",
    "# This notebook will process the combined and aggregated variables across all locations\n",
    "# Two datasets can be analyzed, with and without location-specific outlier capping\n",
    "#df_3 = pd.read_excel(file, sheet_name='Without outlier capping')\n",
    "df_3 = pd.read_excel(file, sheet_name='With outlier capping')\n",
    "\n",
    "# Define the location for plots\n",
    "location = 'Southeast Asia'\n",
    "\n",
    "# Set the Date variable as index, round and sort\n",
    "df_3.set_index(\"Date\", inplace=True)\n",
    "df_3 = df_3.round(4).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd010d4-30ac-4c21-b878-e2a3c56b7948",
   "metadata": {},
   "source": [
    "## 2. Data Exploration\n",
    "This notebook will address the combined and aggregated environmental variables used for the modeling. Initially, the data is deeply explored to check for inconsistencies and analyzed using statistical techniques to further investigate the distribution and structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135c5b47-8aac-44b6-89a5-ecd4ab35c349",
   "metadata": {},
   "source": [
    "#### The combined and aggregated environmental variables for the modeling\n",
    "\n",
    "| Variable           | Unit                 | Description                                                              |\n",
    "|--------------------|----------------------|--------------------------------------------------------------------------|\n",
    "| Min_Daily_Prcp     | Millimeters (mm)     | Minimum daily precipitation                                               |\n",
    "| Max_Daily_Prcp     | Millimeters (mm)     | Maximum daily precipitation                                               |\n",
    "| Monthly_Avg_Prcp   | Millimeters (mm)     | Monthly average precipitation based on daily temperature reporting       |\n",
    "| Monthly_Total_Prcp | Millimeters (mm)     | Monthly total precipitation based on cumulated daily precipitation        |\n",
    "| Monthly_Avg_Temp   | Degrees Celsius (°C) | Monthly average temperature based on daily temperature reporting          |\n",
    "| Min_Daily_Temp     | Degrees Celsius (°C) | Minimum daily temperature of the respective month                         |\n",
    "| Max_Daily_Temp     | Degrees Celsius (°C) | Maximum daily temperature of the respective month                         |\n",
    "| Min_Average_Temp   | Degrees Celsius (°C) | Minimum value of the average daily temperature                            |\n",
    "| Max_Average_Temp   | Degrees Celsius (°C) | Maximum value of the average daily temperature                            |\n",
    "| N_Raining_Days     | Degrees Celsius (°C) | Cumulative number of raining days of the respective month                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34477307-4ef1-482a-b23f-05ec2def31da",
   "metadata": {},
   "source": [
    "### a. df_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67c0929-c861-42cb-aa8d-0355a64da138",
   "metadata": {},
   "source": [
    "#### Exploratory functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2782ce5-06a0-4d5a-b557-fb6fc54877dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the data types and structure using exploratory functions\n",
    "df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecd162b-ddd5-4e19-9689-7259d593cd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the data types and structure using exploratory functions\n",
    "df_3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8642c0ee-ef51-4244-a816-420894931dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the data types and structure using exploratory functions\n",
    "df_3.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfd88fd-d615-408d-bd57-0b6a91aab6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Besides data visualization, there are multiple tests to verify if a time series is stationary\n",
    "# In the following, the Augmented Dickey-Fuller (ADF) test is performed\n",
    "# Inspiration: https://medium.com/@JDEconomics/how-to-test-for-stationarity-in-time-series-data-using-python-44d82890aa9d\n",
    "# Understanding trends and seasonality is fundamental as they provide insights into long-term patterns and recurring fluctuations\n",
    "# Further techniques such as decomposition, differencing, and detrending can be utilized, facilitating the analysis of stationary data\n",
    "\n",
    "# Specifying the subset of variables without \"Min_Daily_Prcp\" due to being constant\n",
    "environmental_variables = [\n",
    "    \"Max_Daily_Prcp\", \"Monthly_Avg_Prcp\", \"Monthly_Total_Prcp\",\n",
    "    \"Monthly_Avg_Temp\", \"Min_Daily_Temp\", \"Max_Daily_Temp\",\n",
    "    \"Min_Average_Temp\", \"Max_Average_Temp\", \"N_Raining_Days\"\n",
    "]\n",
    "\n",
    "# List to store non-stationary variables\n",
    "non_stationary = []\n",
    "\n",
    "# Perform ADF test for selected variables\n",
    "for variable in environmental_variables:\n",
    "    result = adfuller(df_3[variable])\n",
    "    print(f'{variable} shows a p-value of: {result[1]:0.3f}') # Rounded here, therefore shows 0.000 in some instances\n",
    "    if result[1] <= 0.05:\n",
    "        print('Test Result: \\033[92mStationary\\033[0m') \n",
    "    else:\n",
    "        print('Test Result: \\033[91mNon-Stationary\\033[0m')\n",
    "        non_stationary.append(variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dbe902-8ec2-47b6-9160-3e1cda7055dd",
   "metadata": {},
   "source": [
    "#### Result\n",
    "\n",
    "All variables were determined to be stationary; consequently, statistical characteristics did not change over time. No further methods are required to force stationarity, which is good because changing patterns and external forces such as climate change are relevant to the underlying project. Non-stationarity could harm model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47ff1d4-a4a7-442f-b965-2ca94e8857f3",
   "metadata": {},
   "source": [
    "#### Exploratory visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca78327-1c7d-4d3e-99d6-3c71339d573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative target variable Incidence\n",
    "# Interesting but considered careful interpretation because years vary in data availability\n",
    "# With 2016 including data points from six locations, 2023 from one, and 2017-2020 being supported by 17 locations\n",
    "\n",
    "# Group by Month and Year, then sum the Incidence for each group\n",
    "cumulative_incidence = df_3.groupby(['Year', 'Month'])['Incidence'].sum().reset_index()\n",
    "\n",
    "# Increase figure size\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot\n",
    "plt.plot(cumulative_incidence['Year'], cumulative_incidence['Incidence'], marker='o', linestyle='-')\n",
    "plt.title('Cumulative Incidence Over Time', fontsize=18)\n",
    "plt.xlabel('Year', fontsize=14)\n",
    "plt.ylabel('Cumulative Incidence', fontsize=14)\n",
    "plt.xticks()\n",
    "plt.yticks()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbd8055-83a1-4c1f-abe4-f11d2bb2448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To drive interpretability, the mean incidence is displayed\n",
    "\n",
    "# Calculate yearly mean incidence\n",
    "yearly_mean_incidence = df_3.groupby('Year')['Incidence'].mean().reset_index()\n",
    "\n",
    "# Increase figure size\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot\n",
    "plt.plot(yearly_mean_incidence['Year'], yearly_mean_incidence['Incidence'], marker='o', linestyle='-')\n",
    "plt.title('Yearly Mean Incidence', fontsize=18)\n",
    "plt.xlabel('Year', fontsize=14)\n",
    "plt.ylabel('Incidence', fontsize=14)\n",
    "plt.xticks()\n",
    "plt.yticks()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea16d0e-6d8a-4208-aa10-56d0ded6a962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for univariate analysis\n",
    "# Draw a histogram for each numerical variable to explore the general data distribution, structure, and detect outliers of df_3\n",
    "# Subsequentally, data will be scaled. Therefore, outlier treatment is significant \n",
    "\n",
    "# Append the new environmental variable Min_Daily_Temp for the overall exploratory analysis\n",
    "environmental_variables.append(\"Min_Daily_Prcp\")\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "fig, axes = plt.subplots(4, math.ceil(len(environmental_variables) / 4), figsize=(20, 25))\n",
    "\n",
    "for ax, feat in zip(axes.flatten(), environmental_variables):\n",
    "    ax.set_ylabel('Count')\n",
    "    # Plot histogram\n",
    "    counts, bins, patches = ax.hist(df_3[feat].dropna(), alpha=0.6) \n",
    "    \n",
    "    # Adding KDE and statistics\n",
    "    sns.histplot(data=df_3, x=feat, bins=bins, kde=True, ax=ax, line_kws={'lw': 3})\n",
    "    \n",
    "    # Adding annotations for counts\n",
    "    for count, patch in zip(counts, patches):\n",
    "        ax.annotate(str(int(count)), xy=(patch.get_x() + patch.get_width() / 2, patch.get_height()),\n",
    "                    xytext=(0, 5), textcoords='offset points',\n",
    "                    ha='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Adding mean and standard deviation text\n",
    "    ax.text(0.725, 0.9, f'$\\mu={df_3[feat].mean():0.2f}$\\n$\\sigma={df_3[feat].std():0.2f}$',\n",
    "            transform=ax.transAxes, fontsize=15, verticalalignment='top', color='white',\n",
    "            bbox=dict(boxstyle='round'))\n",
    "    \n",
    "plt.suptitle(f\"{location}: Monthly Environmental Variables' Histograms\", fontsize=18)\n",
    "plt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05, hspace=0.4, wspace=0.4)\n",
    "plt.savefig(os.path.join(f\"{location}_monthly_environmental_variables_histograms.png\"), dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b860e50-409f-4d2c-9925-0e4592d9d149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for univaraite analysis\n",
    "# Draw a histogram for each numerical column to explore the general distribution, structure, and detect outliers of df_3\n",
    "# Subsequentally, data will be scaled. Therefore, outlier treatment is significant \n",
    "# Inspiration: https://medium.com/analytics-vidhya/how-to-remove-outliers-for-machine-learning-24620c4657e8\n",
    "\n",
    "fig, axes = plt.subplots(4, math.ceil(len(environmental_variables) / 4), figsize=(20, 25))\n",
    "\n",
    "for ax, feat in zip(axes.flatten(), environmental_variables):\n",
    "    sns.boxplot(x=df_3[feat], ax=ax, color=\"b\")\n",
    "    \n",
    "    # Adding mean and standard deviation text\n",
    "    ax.text(0.75, 0.9, f'$\\mu={df_3[feat].mean():0.2f}$\\n$\\sigma={df_3[feat].std():0.2f}$',\n",
    "            transform=ax.transAxes, fontsize=15, verticalalignment='top', color='white',\n",
    "            bbox=dict(boxstyle='round', facecolor='b'))\n",
    "    \n",
    "plt.suptitle(f\"{location}: Monthly Environmental Variables' Box Plots\", fontsize=18)\n",
    "plt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05, hspace=0.4, wspace=0.4)\n",
    "plt.savefig(os.path.join(f\"{location}_monthly_environmental_variables_boxplots.png\"), dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afaf433-94cd-4fd4-b78c-193bad262a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check rows of the strongest outliers of Monthly_Total_Prcp, that seem extrem\n",
    "df_3.sort_values(by='Monthly_Total_Prcp', ascending=False).head(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b204364-f6ce-4327-97a0-06f959642441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line plot of the df_3 combined variables\n",
    "\n",
    "fig, axes = plt.subplots(4, math.ceil(len(environmental_variables) / 4), figsize=(20, 23))\n",
    "\n",
    "for ax, feat in zip(axes.flatten(), environmental_variables):\n",
    "    ax.set_ylabel('Value')\n",
    "    # Plot line plot\n",
    "    ax.plot(df_3[feat], color=\"g\", alpha=0.6) \n",
    "    \n",
    "    # Adding mean and standard deviation text\n",
    "    ax.text(0.725, 0.9, f'$\\mu={df_3[feat].mean():0.2f}$\\n$\\sigma={df_3[feat].std():0.2f}$',\n",
    "            transform=ax.transAxes, fontsize=15, verticalalignment='top', color='white',\n",
    "            bbox=dict(boxstyle='round', facecolor='g'))\n",
    "    \n",
    "    ax.set_title(feat, fontsize=12)\n",
    "    \n",
    "plt.suptitle(f\"{location}: Monthly Environmental Variables' Line Plots\", fontsize=18)\n",
    "plt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05, hspace=0.4, wspace=0.4)\n",
    "plt.savefig(os.path.join(f\"{location}_monthly_environmental_variables_lineplots.png\"), dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c942c58a-a180-4c43-b1e0-19617ccd4a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonality and trend exploration - not applied here as variables are stationary\n",
    "# Perform seasonal decomposition using the list of non-stationary variables\n",
    "\n",
    "# Perform seasonal decomposition and plot for each non-stationary variable\n",
    "for variable in non_stationary:\n",
    "    result = seasonal_decompose(df_3[variable], model='multiplicative')\n",
    "    \n",
    "    # Access the components of the decomposition\n",
    "    trend = result.trend\n",
    "    seasonal = result.seasonal\n",
    "    residual = result.resid\n",
    "    \n",
    "    # Plot the components for the current variable\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.suptitle(f\"Seasonal Decomposition - {variable}\", fontsize=16)  # Heading for all four plots\n",
    "    \n",
    "    plt.subplot(4, 1, 1)\n",
    "    plt.plot(df_3.index, df_3[variable], label='Original', color='orange')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    plt.subplot(4, 1, 2)\n",
    "    plt.plot(df_3.index, trend, label='Trend', color='blue')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    plt.subplot(4, 1, 3)\n",
    "    plt.plot(df_3.index, seasonal, label='Seasonal', color='red')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    plt.subplot(4, 1, 4)\n",
    "    plt.plot(df_3.index, residual, label='Residual', color='green')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(f\"{location}_seasonal_decomposition.png\"), dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ac3842-3e1f-4b39-9e77-8b622ff5800a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bivariate Analysis with the target variable\n",
    "\n",
    "# Iterate the environmental variables\n",
    "for variable in environmental_variables:\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Plot environmental variable against target variable\n",
    "    sns.lineplot(data=df_3, x='Date', y=variable, color='green', label=variable)\n",
    "    \n",
    "    # Plot target variable Incidence\n",
    "    ax2 = plt.twinx()\n",
    "    sns.lineplot(data=df_3, x='Date', y='Incidence', color='orange', ax=ax2, label='Incidence')\n",
    "\n",
    "    plt.title(f\"{location}: Bivariate Analysis of Monthly Incidence and {variable}\", fontsize=18)\n",
    "    plt.xlabel('Date')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(False)\n",
    "    plt.savefig(os.path.join(f\"{location}_bivariate_analysis.png\"), dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70355e0-2226-485d-810f-1b86b6c6c4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for seasonal, monthly dependencies using a visualization technique\n",
    "\n",
    "# Grouping the data by year for extraction\n",
    "grouped_data = df_3.groupby('Year')['Monthly_Total_Prcp'].apply(list)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for year, values in grouped_data.items():\n",
    "    months = df_3[df_3['Year'] == year]['Month']\n",
    "    plt.plot(months, values, label=str(year))\n",
    "\n",
    "plt.title(f\"{location}: Observing Monthly Dependencies using Yearly Data\", fontsize=18)\n",
    "plt.xlabel('Month of the Year')\n",
    "plt.ylabel('Value')\n",
    "plt.legend(title='Year')\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(f\"{location}_seasonal_dependencies_weekly_incidence.png\"), dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bd43e6-c595-4ac2-a7d6-40db03c19a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the correlation using a correlation one-sided matrix\n",
    "\n",
    "# Select the variables to compute the correlation\n",
    "correlation_variables = [\"Min_Daily_Prcp\", \"Max_Daily_Prcp\", \"Monthly_Avg_Prcp\",\n",
    "                         \"Monthly_Total_Prcp\", \"Monthly_Avg_Temp\", \"Min_Daily_Temp\", \n",
    "                         \"Max_Daily_Temp\", \"Min_Average_Temp\", \"Max_Average_Temp\", \n",
    "                         \"N_Raining_Days\", \"Incidence Rate\"]\n",
    "\n",
    "# Create a correlation matrix\n",
    "correlation_matrix = df_3[correlation_variables].corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "# Heatmap creation\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, mask=mask, cmap='coolwarm', annot=True, fmt=\"0.2f\", linewidths=0.5)\n",
    "plt.title(f\"Correlation Matrix of Incidence Rate and Monthly Environmental Variables\", fontsize=18)\n",
    "plt.savefig(os.path.join(f\"{location}_correlation_matrix_monthly_1.png\"), dpi=200, bbox_inches='tight')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e145437-6f42-41d2-bf5a-78195ac27c5c",
   "metadata": {},
   "source": [
    "#### Result\n",
    "\n",
    "The feature Min_Daily_Prcp, which represents the minimum daily precipitation, shows limited significance with 0 mm in most cases. In addition, the variables Monthly_Average_Prcp, Min_Average_Temp, and Max_Average_Temp might lead to multicollinearity due to high correlation, which adds redundancy. Overall, the correlation matrix shows low significance with the target variable.\n",
    "\n",
    "The univariate analysis shows remaining outliers for the dataset with location-specific outlier capping. Consequently, aside from data aggregation and the capping of relative outliers, the model performance might benefit from addressing extremes on a multi-location level. This will be tested at a later stage. \n",
    "\n",
    "Considering the total dengue infection cycle lasts 4-7 weeks, the bivariate analysis does not reveal a strong relationship with the target variable. An association of seasonality can not be derived. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2b12ae-ba8f-4648-a138-3192951a79f4",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "In the first data preprocessing, missing values of the raw data have been imputed. In notebook 2, outliers were capped in location-specific datasets. This step performs further preprocessing, such as feature deletion, outlier detection and removal on a multi-location level, encoding, partition, and scaling.  Additionally, feature engineering is performed by creating lags of the environmental variables.\n",
    "\n",
    "Having identified some extreme outliers in the EDA, robust scaling will be interesting because it is robust to outliers. However, the MinMax scaler will also be applied to compare modeling outcomes as this thesis focuses on applying a multitude of methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a071a67-5276-431d-a24b-c634b8b3223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing environmental variables\n",
    "environmental_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3ba074-375e-433b-bbfa-8c6c53b393f8",
   "metadata": {},
   "source": [
    "### a. Feature Removal\n",
    "\n",
    "Removing irrelevant features that are identified irrlevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dd5e59-2ed8-4d8c-9306-8af429b350f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Min_Daily_Prcp because of insufficient interpretability\n",
    "# Removing Monthly_Avg_Prcp, Min_Average_Temp, Max_Average_Temp due to high correlation\n",
    "\n",
    "df_3 = df_3.drop(columns=['Min_Daily_Prcp', 'Monthly_Avg_Prcp', 'Min_Average_Temp', 'Max_Average_Temp'])\n",
    "\n",
    "# Simultaneous removal of the list\n",
    "for var in ['Min_Daily_Prcp', 'Monthly_Avg_Prcp', 'Min_Average_Temp', 'Max_Average_Temp']:\n",
    "    if var in environmental_variables:\n",
    "        environmental_variables.remove(var)\n",
    "\n",
    "# Print updated list\n",
    "environmental_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d1f9f5-6db7-4d9a-ba8c-86925979b648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the correlation using a correlation one-sided matrix with the updated variables\n",
    "\n",
    "# Select the variables to compute the correlation\n",
    "correlation_variables = [\"Max_Daily_Prcp\", \"Monthly_Total_Prcp\", \"Min_Daily_Temp\", \n",
    "                         \"Max_Daily_Temp\", \"Monthly_Avg_Temp\", \n",
    "                         \"N_Raining_Days\", \"Incidence Rate\"]\n",
    "\n",
    "# Create a correlation matrix\n",
    "correlation_matrix = df_3[correlation_variables].corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "# Heatmap creation\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, mask=mask, cmap='coolwarm', annot=True, fmt=\"0.2f\", linewidths=0.5)\n",
    "plt.title(f\"Correlation Matrix of Incidence Rate and Monthly Environmental Variables\", fontsize=18)\n",
    "plt.savefig(os.path.join(f\"{location}_correlation_matrix_monthly_2.png\"), dpi=200, bbox_inches='tight')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9422a91d-8ab1-437c-a1dd-43a6d7c2a9d5",
   "metadata": {},
   "source": [
    "### b. Outlier Detection \n",
    "\n",
    "The main objective is to find a balance by reducing noise and allowing the dynamic nature of the environmental features. As emphasized, extreme weather events such as flushing tremendously affect mosquito habitat, lifecycle, and disease transmission. Accordingly, methods like outlier removal and trimming through IQR were disregarded.\n",
    "\n",
    "At first, the standard deviation method is conducted to explore outliers with a standard deviation far from the mean. Additionally, the z-score method is utilized for detection, standardizing the data, and identifying outliers based on their distance from the mean in terms of standard deviations. Outlier capping would not be beneficial for the identification as it determines outliers for all variables within a specified percentile. Therefore, it simply defines the most profound values. Nevertheless, outlier capping is an elegant approach for outlier treatment, preserving integrity, improving interpretability, and avoiding overfitting. There are two approaches: Floor capping values below a certain threshold and ceiling replacing values above the threshold.\n",
    "\n",
    "- Source Capping: https://medium.com/analytics-vidhya/outlier-detection-in-machine-learning-382557c775aa\n",
    "- Source IQR: https://machinelearningmastery.com/how-to-use-statistics-to-identify-outliers-in-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a43e88-3d44-479c-b766-e165b4ee25c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the standard deviation method to detect outliers\n",
    "\n",
    "# Calculate mean and standard deviation only for environmental variables\n",
    "means = df_3[environmental_variables].mean()\n",
    "stds = df_3[environmental_variables].std()\n",
    "\n",
    "# Defining the threshold\n",
    "threshold = 4\n",
    "\n",
    "# Identify outliers for each environmental variable\n",
    "outliers = {}\n",
    "for variable in environmental_variables:\n",
    "    outlier_indices = np.abs(df_3[variable] - means[variable]) > threshold * stds[variable]\n",
    "    outliers[variable] = df_3.loc[outlier_indices, variable].tolist()\n",
    "\n",
    "print(\"Outliers identified using the Standard Deviation Method:\")\n",
    "for variable, values in outliers.items():\n",
    "    values.sort()\n",
    "    print(f\"{variable}: {values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc848ac1-11b0-489f-b147-df508095a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold for z-score beyond which a data point is considered an outlier\n",
    "\n",
    "# Calculate z-scores for each environmental variable\n",
    "z_scores = (df_3[environmental_variables] - df_3[environmental_variables].mean()) / df_3[environmental_variables].std()\n",
    "\n",
    "# Defining the threshold\n",
    "threshold = 4\n",
    "\n",
    "# Identify outliers for each environmental variable\n",
    "outliers = {}\n",
    "for variable in environmental_variables:\n",
    "    outlier_indices = np.abs(z_scores[variable]) > threshold\n",
    "    outliers[variable] = df_3.loc[outlier_indices, variable].tolist()\n",
    "\n",
    "print(\"Outliers identified using the Z-Score Method:\")\n",
    "for variable, values in outliers.items():\n",
    "    values.sort()\n",
    "    print(f\"{variable}: {values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d85cbfb-0d2c-4243-b89b-6636e3ad5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check quantiles before applying outlier capping\n",
    "# Quartiles divide the data into four equal parts with Q2 being the median\n",
    "\n",
    "# Dictionary to store outliers for each variable\n",
    "outliers = {}\n",
    "\n",
    "# Iterate over each variable\n",
    "for variable in environmental_variables:\n",
    "    \n",
    "    # Calculate the first and third quartile of each variable\n",
    "    Q1 = df_3[variable].quantile(0.10)\n",
    "    Q3 = df_3[variable].quantile(0.90)\n",
    "            \n",
    "    # Calculating the IQR range\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Calculating the lower and upper bound/whisker\n",
    "    lower_bound = Q1 - (1.5 * IQR)\n",
    "    upper_bound = Q3 + (1.5 * IQR)\n",
    "\n",
    "    # Identify outliers\n",
    "    outlier_indices = (df_3[variable] < lower_bound) | (df_3[variable] > upper_bound)\n",
    "    outliers[variable] = df_3.loc[outlier_indices, variable].tolist()\n",
    "\n",
    "# Print outliers for each variable outside the loop\n",
    "print(\"Outliers identified using the IQR Method:\")\n",
    "for variable, values in outliers.items():\n",
    "    values.sort()\n",
    "    print(f\"{variable}: {values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee66f21-42b1-4e22-8c98-5aee9b8bd6b9",
   "metadata": {},
   "source": [
    "### c. Outlier Removal\n",
    "\n",
    "Depending on the use case, the following function needs to be commented / not commented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7809ccab-0150-43d9-90ba-dedb2e76efe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capping Functions\n",
    "# https://wellsr.com/python/outlier-data-handling-with-python/#:~:text=Capping%20Outliers%20using%20Fixed%20Quantiles&text=In%20such%20cases%20you%20can,the%20records%20in%20the%20dataset.\n",
    "\n",
    "def outlier_capping(data, outlier_variables):\n",
    "    # Create a copy of the original data to avoid modifying it directly\n",
    "    # data = data.copy()\n",
    "\n",
    "    # Loops through each feature with outliers.\n",
    "    for variable in outlier_variables:\n",
    "\n",
    "            # Defines the first and third quantile of each feature.\n",
    "            Q1 = data[variable].quantile(0.15)\n",
    "            Q3 = data[variable].quantile(0.85)\n",
    "\n",
    "            # Calculating the IQR range.\n",
    "            IQR = Q3 - Q1\n",
    "\n",
    "            # Calculating the lower and upper bound/whisker\n",
    "            lower_bound = Q1 - (1.5 * IQR)\n",
    "            upper_bound = Q3 + (1.5 * IQR)\n",
    "\n",
    "            # Replaces all values below the lower bound and\n",
    "            # above the upper bound with the lower or upper bound values.\n",
    "            data[variable] = np.where(data[variable] > upper_bound, upper_bound,\n",
    "                                        np.where(data[variable] < lower_bound, lower_bound,\n",
    "                                                  data[variable]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a1daa0-58a8-48b2-8e8f-6e4f166d6985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outlier_capping(df_3, environmental_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2581f3a6-84fc-45c3-9672-846a57dd034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram to control outlier removal\n",
    "\n",
    "fig, axes = plt.subplots(3, math.ceil(len(environmental_variables) / 4), figsize=(20, 23))\n",
    "\n",
    "for ax, feat in zip(axes.flatten(), environmental_variables):\n",
    "    ax.set_ylabel('Count')\n",
    "    # Plot histogram\n",
    "    counts, bins, patches = ax.hist(df_3[feat].dropna(), alpha=0.6) \n",
    "    \n",
    "    # Adding KDE and statistics\n",
    "    sns.histplot(data=df_3, x=feat, bins=bins, kde=True, ax=ax, line_kws={'lw': 3})\n",
    "    \n",
    "    # Adding annotations for counts\n",
    "    for count, patch in zip(counts, patches):\n",
    "        ax.annotate(str(int(count)), xy=(patch.get_x() + patch.get_width() / 2, patch.get_height()),\n",
    "                    xytext=(0, 5), textcoords='offset points',\n",
    "                    ha='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Adding mean and standard deviation text\n",
    "    ax.text(0.725, 0.9, f'$\\mu={df_3[feat].mean():.2f}$\\n$\\sigma={df_3[feat].std():.2f}$',\n",
    "            transform=ax.transAxes, fontsize=15, verticalalignment='top', color='white',\n",
    "            bbox=dict(boxstyle='round'))\n",
    "    \n",
    "plt.suptitle(f\"Monthly Environmental Variables' Histograms after Outlier Capping\", fontsize=18)\n",
    "plt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05, hspace=0.4, wspace=0.4)\n",
    "plt.savefig(os.path.join(f\"{location}_monthly_environmental_variables_histograms_outlier_removal.png\"), dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4739333-b6ef-4ef5-b9b6-723f863cc74b",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "Outlier detection was performed for each location using the z-score and standard deviation method. Observations beyond a threshold of 2 or 3 standard deviations from the mean are considered outliers. Here, I experimented with a threshold of 3 and 4 and concluded that 4 was a better fit. The outcome of the outlier detection was compared with the IQR method. Visualizations were used to support the assessment. In conclusion, the 15/85 split was chosen to maintain analytical integrity while tolerating the complexities of environmental data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "496bbb76-39fb-484d-a70b-2f9b602b4567",
   "metadata": {},
   "source": [
    "### d. Feature Creation\n",
    "\n",
    "Having defined relevant environmental variables in the previous sections, the objective here is to create lagged variables for optimal time-series forecasting. At first, the lagged intervals will be determined. As discussed in the literature review, the total dengue infection cycle lasts 4-7 weeks. Moreover, as elaborated, environmental factors have an immense influence on the mosquito habitat and lifecycle. Consequently, lagged variables will be established for each environmental factor at multiple intervals prior to the current observation, considering, that the data is aggregated on a monthly scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f596505-701a-4074-9af6-e22678fef24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of lagged features through transformation\n",
    "# Lag intervals from 1 to 6 months\n",
    "# Meaning missing values will be created in the first five instances as the data is not given\n",
    "# Rows with missing values will be excluded to avoid subsequent issues\n",
    "\n",
    "# Define interval\n",
    "lagged_intervals = range(1, 7)\n",
    "new_features = []\n",
    "\n",
    "# Create lags\n",
    "for variable in environmental_variables:\n",
    "    for lag in lagged_intervals:\n",
    "        new_feature_name = f\"{variable}_lag_{lag}\"\n",
    "        df_3[new_feature_name] = df_3[variable].shift(lag)\n",
    "        new_features.append(new_feature_name)\n",
    "\n",
    "# Update environmental_variables\n",
    "environmental_variables.extend(new_features)\n",
    "\n",
    "# Drop rows with missing values as they are represented in the lags\n",
    "df_3.dropna(inplace=True)\n",
    "\n",
    "# Visualize the engineered data \n",
    "df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d109f791-7ca8-4066-a183-664001b70b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the correlation of lagged variables using a correlation one-sided matrix\n",
    "\n",
    "# Select the variables to compute the correlation\n",
    "correlation_variables = [\"Max_Daily_Prcp\", \"Max_Daily_Prcp_lag_1\", \"Max_Daily_Prcp_lag_2\",\n",
    "                         \"Max_Daily_Prcp_lag_3\", \"Max_Daily_Prcp_lag_4\", \"Max_Daily_Prcp_lag_5\",\n",
    "                         \"Max_Daily_Prcp_lag_6\", \"Incidence Rate\"]\n",
    "\n",
    "# Create a correlation matrix\n",
    "correlation_matrix = df_3[correlation_variables].corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "# Heatmap creation\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, mask=mask, cmap='coolwarm', annot=True, fmt=\"0.2f\", linewidths=0.5)\n",
    "plt.title(f\"Correlation Matrix of Incidence Rate and Max_Daily_Prcp Lagged Variables\", fontsize=18)\n",
    "plt.savefig(os.path.join(f\"{location}_correlation_matrix_lagged.png\"), dpi=200, bbox_inches='tight')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02244027-559c-4e0c-aaaa-9cab76bd822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the correlation of lagged variables using a correlation one-sided matrix\n",
    "\n",
    "# Select the variables to compute the correlation\n",
    "correlation_variables = [\"Monthly_Total_Prcp\", \"Monthly_Total_Prcp_lag_1\", \"Monthly_Total_Prcp_lag_2\",\n",
    "                         \"Monthly_Total_Prcp_lag_3\", \"Monthly_Total_Prcp_lag_4\", \"Monthly_Total_Prcp_lag_5\",\n",
    "                         \"Monthly_Total_Prcp_lag_6\", \"Incidence Rate\"]\n",
    "\n",
    "# Create a correlation matrix\n",
    "correlation_matrix = df_3[correlation_variables].corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "# Heatmap creation\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, mask=mask, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=.5)\n",
    "plt.title(f\"Correlation Matrix of Incidence Rate and Monthly_Total_Prcp Lagged Variables\", fontsize=18)\n",
    "plt.savefig(os.path.join(f\"{location}_correlation_matrix_lagged.png\"), dpi=200, bbox_inches='tight')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a2d892-3435-48bd-a7d1-90fe87c5528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the correlation of lagged variables using a correlation one-sided matrix\n",
    "\n",
    "# Select the variables to compute the correlation\n",
    "correlation_variables = [\"Min_Daily_Temp\", \"Min_Daily_Temp_lag_1\", \"Min_Daily_Temp_lag_2\",\n",
    "                         \"Min_Daily_Temp_lag_3\", \"Min_Daily_Temp_lag_4\", \"Min_Daily_Temp_lag_5\",\n",
    "                         \"Min_Daily_Temp_lag_6\", \"Incidence Rate\"]\n",
    "\n",
    "# Create a correlation matrix\n",
    "correlation_matrix = df_3[correlation_variables].corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "# Heatmap creation\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, mask=mask, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=.5)\n",
    "plt.title(f\"Correlation Matrix of Incidence Rate and Min_Daily_Temp Lagged Variables\", fontsize=18)\n",
    "plt.savefig(os.path.join(f\"{location}_correlation_matrix_lagged.png\"), dpi=200, bbox_inches='tight')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db066628-238a-462d-8d69-f646da35f9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the correlation of lagged variables using a correlation one-sided matrix\n",
    "\n",
    "# Select the variables to compute the correlation\n",
    "correlation_variables = [\"Max_Daily_Temp\", \"Max_Daily_Temp_lag_1\", \"Max_Daily_Temp_lag_2\",\n",
    "                         \"Max_Daily_Temp_lag_3\", \"Max_Daily_Temp_lag_4\", \"Max_Daily_Temp_lag_5\",\n",
    "                         \"Max_Daily_Temp_lag_6\", \"Incidence Rate\"]\n",
    "\n",
    "# Create a correlation matrix\n",
    "correlation_matrix = df_3[correlation_variables].corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "# Heatmap creation\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, mask=mask, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=.5)\n",
    "plt.title(f\"Correlation Matrix of Incidence Rate and Max_Daily_Temp Lagged Variables\", fontsize=18)\n",
    "plt.savefig(os.path.join(f\"{location}_correlation_matrix_lagged.png\"), dpi=200, bbox_inches='tight')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78beebf8-4f73-4988-b786-041f67f1d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the correlation of lagged variables using a correlation one-sided matrix\n",
    "\n",
    "# Select the variables to compute the correlation\n",
    "correlation_variables = [\"Monthly_Avg_Temp\", \"Monthly_Avg_Temp_lag_1\", \"Monthly_Avg_Temp_lag_2\",\n",
    "                         \"Monthly_Avg_Temp_lag_3\", \"Monthly_Avg_Temp_lag_4\", \"Monthly_Avg_Temp_lag_5\",\n",
    "                         \"Monthly_Avg_Temp_lag_6\", \"Incidence Rate\"]\n",
    "\n",
    "# Create a correlation matrix\n",
    "correlation_matrix = df_3[correlation_variables].corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "# Heatmap creation\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, mask=mask, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=.5)\n",
    "plt.title(f\"Correlation Matrix of Incidence Rate and Monthly_Avg_Temp Lagged Variables\", fontsize=18)\n",
    "plt.savefig(os.path.join(f\"{location}_correlation_matrix_lagged.png\"), dpi=200, bbox_inches='tight')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c7fcb-25e4-477a-ae44-36fe07913f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many features are established after feature engineering\n",
    "len(environmental_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec147e2-6a8e-4ad4-bb86-c13d0548d1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print list with lagged variables\n",
    "environmental_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e939be-441b-46ba-928a-56218f542b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of the correlation of incidence with the environmental variables and their lagged observations\n",
    "\n",
    "# Calculate the correlation matrix for the selected variables\n",
    "correlation_matrix = df_3[environmental_variables + ['Incidence Rate']].corr()\n",
    "\n",
    "# Filter the correlation matrix to include only the correlations with the target variable\n",
    "filtered_corr_matrix = correlation_matrix.loc[environmental_variables, 'Incidence Rate']\n",
    "\n",
    "# Sort the correlation values in descending order\n",
    "sorted_corr_matrix = filtered_corr_matrix.sort_values(ascending=False)\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(12, 18))\n",
    "ax = sns.heatmap(sorted_corr_matrix.to_frame(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5, linecolor='black')\n",
    "ax.set_title('Correlation of Incidence Rate with Monthly Environmental Variables and Lagged Observations', fontsize=18)\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('Environmental Variables', fontsize=14)\n",
    "ax.set_xticklabels(['Incidence Rate'], fontsize=12)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(os.path.join(f\"{location}_correlation_heatmap_all_variables.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f78b57-7d41-4402-b648-97264f6ade4a",
   "metadata": {},
   "source": [
    "### e. Data Encoding\n",
    "Encoding of categorical variable \"Name\". Source of inspiration: https://medium.com/aiskunks/categorical-data-encoding-techniques-d6296697a40f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7bf15f-d189-47cc-b70b-278554070910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the variable Name with LabelEncoder\n",
    "df_3['Location Code'] = LabelEncoder().fit_transform(df_3['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d748cc-0f67-4f30-a24e-6647dd216436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print location names and their respective location codes\n",
    "df_3[['Name', 'Location Code']].drop_duplicates().sort_values('Location Code')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e35b6d-5ac9-46e8-9504-ae9c1089c766",
   "metadata": {},
   "source": [
    "### f. Data Partition\n",
    "\n",
    "Before normalizing data by applying different scaling methods, data is split to compare scaled and unscaled data in the modeling section directly. The aim is to carefully split the time-series data with respect to the index Date to avoid data leakage. Different approaches will be tested, with one holistic strategy and a location-specific individual approach. The later applied time-series cross-validation aligns with an expanding window training approach.\n",
    "\n",
    "Sources of inspiration:\n",
    "- https://medium.com/@content_33167/sampling-techniques-for-time-series-data-analysis-8d76e423baed\n",
    "- https://towardsdatascience.com/time-based-cross-validation-d259b13d42b8\n",
    "- https://medium.com/@soumyachess1496/cross-validation-in-time-series-566ae4981ce4\n",
    "- https://medium.com/@Stan_DS/timeseries-split-with-sklearn-tips-8162c83612b9\n",
    "- https://forecastegy.com/posts/time-series-cross-validation-python/\n",
    "- https://tomerkatzav.medium.com/split-time-series-dataset-826b7dc39cd9\n",
    "- https://towardsdatascience.com/cross-validation-explained-evaluating-estimator-performance-e51e5430ff85\n",
    "- https://medium.com/eatpredlove/time-series-cross-validation-a-walk-forward-approach-in-python-8534dd1db51a\n",
    "- https://gghantiwala.medium.com/can-using-a-different-way-to-split-your-time-series-data-improve-model-performance-3f5042f13dff"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e8cbaf6-6f0f-4b14-b3c6-fc25137795ae",
   "metadata": {},
   "source": [
    "#### Holistic approach\n",
    "\n",
    "Presented here is the aligned time-series of the holistic model. The initially implemented time-based split for each location raised the concern of data leakage due to non-aligned time ranges within the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f294d008-657d-4589-89fe-af120aa9ade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holistic, dependent approach with cut-off\n",
    "\n",
    "# Create lists\n",
    "train_dfs, val_dfs, test_dfs, y_train_dfs, y_val_dfs, y_test_dfs = [], [], [], [], [], []\n",
    "\n",
    "# Define the cutoff dates\n",
    "validation_cutoff = pd.Timestamp('2019-01-01')\n",
    "test_cutoff = pd.Timestamp('2020-01-01')\n",
    "\n",
    "# Perform the split for each unique location\n",
    "for location in df_3['Location Code'].unique():\n",
    "    location_df = df_3[df_3['Location Code'] == location]\n",
    "    features = ['Location Code'] + environmental_variables  # Assume environmental_variables is defined\n",
    "    target = 'Incidence Rate'\n",
    "    \n",
    "    # Select features and target for the current location\n",
    "    X_location = location_df[features]\n",
    "    y_location = location_df[target]\n",
    "\n",
    "    # Split data into train, validation, and test sets based on the cutoff\n",
    "    X_train = X_location[(X_location.index < validation_cutoff)]\n",
    "    X_val = X_location[(X_location.index >= validation_cutoff) & (X_location.index < test_cutoff)]\n",
    "    X_test = X_location[X_location.index >= test_cutoff]\n",
    "\n",
    "    y_train = y_location[(y_location.index < validation_cutoff)]\n",
    "    y_val = y_location[(y_location.index >= validation_cutoff) & (y_location.index < test_cutoff)]\n",
    "    y_test = y_location[y_location.index >= test_cutoff]\n",
    "\n",
    "    # Append the splits to their respective lists\n",
    "    train_dfs.append(X_train)\n",
    "    val_dfs.append(X_val)\n",
    "    test_dfs.append(X_test)\n",
    "    y_train_dfs.append(y_train)\n",
    "    y_val_dfs.append(y_val)\n",
    "    y_test_dfs.append(y_test)\n",
    "    \n",
    "# Concatenate the splits into training, validation, and testing sets\n",
    "X_train, X_val, X_test = pd.concat(train_dfs).sort_index(), pd.concat(val_dfs).sort_index(), pd.concat(test_dfs).sort_index()\n",
    "y_train, y_val, y_test = pd.concat(y_train_dfs).sort_index(), pd.concat(y_val_dfs).sort_index(), pd.concat(y_test_dfs).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05bb5e6-9431-4641-a949-4050b0a4e419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Holistic, dependent approach with cut-off\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot data for each unique location\n",
    "for location in df_3['Location Code'].unique(): \n",
    "    # Plot training data\n",
    "    train_subset = X_train[X_train['Location Code'] == location]\n",
    "    plt.scatter(train_subset.index, [location] * len(train_subset), color='blue', alpha=0.5)\n",
    "    \n",
    "    # Plot validation data\n",
    "    val_subset = X_val[X_val['Location Code'] == location]\n",
    "    plt.scatter(val_subset.index, [location] * len(val_subset), color='green', alpha=0.5)\n",
    "\n",
    "    # Plot testing data\n",
    "    test_subset = X_test[X_test['Location Code'] == location]\n",
    "    plt.scatter(test_subset.index, [location] * len(test_subset), color='red', alpha=0.5)\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Location Code')\n",
    "plt.title('Train-Validation-Test Split by Location Code', fontsize=18)\n",
    "plt.legend(['Train', 'Validation', 'Test'])\n",
    "plt.grid(True)\n",
    "#plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(f\"Train-Validation-Test-Split-by-Location_2.png\"), dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aa64a6-08b0-4bfc-af91-75dd14de9199",
   "metadata": {},
   "source": [
    "#### Individual approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1546a6-6f2c-4e2f-bad5-d887beff595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual, independent approach\n",
    "# New: Reserving the X_train for later using lists\n",
    "#Here, time-based train test split\n",
    "\n",
    "# Prepare lists to hold split data\n",
    "train_dfs, test_dfs, y_train_dfs, y_test_dfs = [], [], [], []\n",
    "\n",
    "# Perform the split for each unique location\n",
    "for location in df_3['Location Code'].unique():\n",
    "    \n",
    "    location_df = df_3[df_3['Location Code'] == location]\n",
    "    features = ['Location Code'] + environmental_variables\n",
    "    target = 'Incidence Rate'\n",
    "    \n",
    "    # Select features and target for the current location\n",
    "    X_location = location_df[features]\n",
    "    y_location = location_df[target]\n",
    "\n",
    "    # Calculate the train size by excluding the last 12 instances, which represents a year\n",
    "    train_size = len(X_location) - 12\n",
    "\n",
    "    # Split the data into train and test sets for the current location\n",
    "    X_train_val, X_test = X_location.iloc[:train_size], X_location.iloc[train_size:]\n",
    "    y_train_val, y_test = y_location.iloc[:train_size], y_location.iloc[train_size:]\n",
    "\n",
    "    # Append the splits to their respective lists\n",
    "    train_dfs.append(X_train_val)\n",
    "    test_dfs.append(X_test)\n",
    "    y_train_dfs.append(y_train_val)\n",
    "    y_test_dfs.append(y_test)\n",
    "    \n",
    "# Concatenate the splits into training and testing sets\n",
    "X_train_val, X_test = pd.concat(train_dfs).sort_index(), pd.concat(test_dfs).sort_index()\n",
    "y_train_val, y_test = pd.concat(y_train_dfs).sort_index(), pd.concat(y_test_dfs).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79e5989-2d0f-42b9-9e1a-ec5842483e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Individual, independent approach\n",
    "\n",
    "# Visualization of the train-validation-test split by location\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot data for each unique location\n",
    "for location in df_3['Location Code'].unique(): \n",
    "    # Plot training data\n",
    "    train_subset = X_train_val[X_train_val['Location Code'] == location]\n",
    "    plt.scatter(train_subset.index, [location] * len(train_subset), color='blue', alpha=0.5)\n",
    "\n",
    "    # Plot testing data\n",
    "    test_subset = X_test[X_test['Location Code'] == location]\n",
    "    plt.scatter(test_subset.index, [location] * len(test_subset), color='red', alpha=0.5)\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Location Code')\n",
    "plt.title('Train-Test Split by Location Code', fontsize=18)\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.grid(True)\n",
    "plt.xticks()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(f\"{location}:Train-Test-Split-for-each-location.png\"), dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07aa272-5ee8-4ca9-8415-a6ac0fef1c80",
   "metadata": {},
   "source": [
    "### g. Data Normalization\n",
    "\n",
    "Having identified some extreme outliers in the EDA, RobustScaler is applied because it is robust to outliers. However, the MinMaxScaler will  be applied to compare modeling outcomes as this thesis focuses on applying a multitude of methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f00b721-479e-4128-b952-1cbea82a0abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for feature scaling\n",
    "\n",
    "def feature_scaling(X_train, X_test, numerical_features, scaler):\n",
    "\n",
    "    # Apply scaling\n",
    "    X_train_scaled = scaler.fit_transform(X_train[numerical_features])\n",
    "    X_test_scaled = scaler.transform(X_test[numerical_features])\n",
    "    \n",
    "    # Convert scaled arrays back to DataFrame, ensuring correct column names and index\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=numerical_features, index=X_train.index)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=numerical_features, index=X_test.index)\n",
    "    \n",
    "    # Combine scaled and non-scaled features\n",
    "    X_train_final = pd.concat([X_train.drop(columns=numerical_features), X_train_scaled], axis=1)\n",
    "    X_test_final = pd.concat([X_test.drop(columns=numerical_features), X_test_scaled], axis=1)\n",
    "    \n",
    "    return X_train_final, X_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b524453b-a276-446f-9212-b1c007502dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for inverse scaling\n",
    "\n",
    "def inverse_feature_scaling(X_train_scaled, X_test_scaled, numerical_features, scaler):\n",
    "\n",
    "    # Inverse scaling\n",
    "    X_train_inverse_scaled = scaler.inverse_transform(X_train_scaled[numerical_features])\n",
    "    X_test_inverse_scaled = scaler.inverse_transform(X_test_scaled[numerical_features])\n",
    "    \n",
    "    # Convert inverse scaled arrays back to DataFrame, ensuring correct column names and index\n",
    "    X_train_inverse_scaled = pd.DataFrame(X_train_inverse_scaled, columns=numerical_features, index=X_train_scaled.index)\n",
    "    X_test_inverse_scaled = pd.DataFrame(X_test_inverse_scaled, columns=numerical_features, index=X_test_scaled.index)\n",
    "    \n",
    "    # Combine inverse scaled and non-scaled features\n",
    "    X_train_final = pd.concat([X_train_scaled.drop(columns=numerical_features), X_train_inverse_scaled], axis=1)\n",
    "    X_test_final = pd.concat([X_test_scaled.drop(columns=numerical_features), X_test_inverse_scaled], axis=1)\n",
    "    \n",
    "    return X_train_final, X_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4332d1a9-9dfd-4cd0-b9ee-1f93fb658e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val_scaled_minmax, X_test_scaled_minmax = feature_scaling(X_train_val, X_test, environmental_variables, MinMaxScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2ee63b-d478-4e3e-9101-5c401ac7c30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val_scaled_robust, X_test_scaled_robust = feature_scaling(X_train_val, X_test, environmental_variables, RobustScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bff51b-2612-4b7e-917b-3f3284035480",
   "metadata": {},
   "source": [
    "## 4. Feature Selection\n",
    "\n",
    "For the feature selection, multiple methods were explored. The main objective is to define significant environmental features that interact with disease transmission to boost model performance. In the following, the chosen methods are presented:\n",
    "\n",
    "\n",
    "| Method                              | Description                                                                                           | Comment                                                                                                                             |\n",
    "|-------------------------------------|-------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| RFE (Recursive Feature Elimination) | The wrapper method eliminates the least significant features iteratively until the optimal subset is achieved. | Regressor and the number of features need to be prior defined, can handle multiple relationships with target variable. |\n",
    "| Forward Feature Selection | This wrapper method iteratively adds the most significant features until the optimal subset is defined. | Regressor and the number of features need to be prior defined, can handle multiple relationships with target variable. |\n",
    "| Random Forest | The filter method estimates feature importance based on how much the tree nodes that use the feature reduce impurity.  | Robust method for feature selection with built-in ranking, capable of handling non-linear relationships Tree-based, therefore, not sensitive to scaling. |\n",
    "| XGBoost | Similar to Random Forest, XGBoost estimates feature importance based on how much the tree nodes that use the feature reduce impurity. | Also a robust method for feature selection, often used in ensemble methods. Sensitive to scaling but less than linear models. |\n",
    "\n",
    "Comment: sklearn.feature_selection.SelectFromModel is a meta-transformer for selecting features based on importance weights.\n",
    "\n",
    "Sources: \n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html\n",
    "- https://scikit-learn.org/stable/modules/feature_selection.html\n",
    "- https://towardsdatascience.com/feature-importance-and-forward-feature-selection-752638849962\n",
    "- https://towardsdatascience.com/feature-selection-using-random-forest-26d7b747597f\n",
    "- https://builtin.com/data-science/feature-importance\n",
    "- https://www.analyticsvidhya.com/blog/2021/04/forward-feature-selection-and-its-implementation/#:~:text=Forward%20feature%20selection%20involves%20iteratively,the%20most%20informative%20features%20incrementally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9379fb-b927-44a6-823c-c89bcb8e89fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First method with Random Forest based on feature importance\n",
    "\n",
    "# Initialize SelectFromModel with RandomForestRegressor\n",
    "rf_selector = SelectFromModel(RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "\n",
    "# Fit the selector on training data\n",
    "rf_selector.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Get and print the selected features\n",
    "rf_features = list(X_train_val.columns[(rf_selector.get_support())])\n",
    "print(rf_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaa3795-e2c8-4b25-ae13-1adcd4582764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print plot\n",
    "\n",
    "# Access the Random Forest model to get the feature importances\n",
    "feature_importances = rf_selector.estimator_.feature_importances_\n",
    "\n",
    "# Get the indices of the features sorted by importance\n",
    "sorted_indices = feature_importances.argsort()[::-1]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Feature Importance using RandomForestRegressor()',fontsize=18)\n",
    "plt.bar(range(X_train_val.shape[1]), feature_importances[sorted_indices], align='center')\n",
    "plt.xticks(range(X_train_val.shape[1]), X_train_val.columns[sorted_indices], rotation=90)\n",
    "plt.ylabel('Importance')\n",
    "plt.subplots_adjust(bottom=0.3)\n",
    "plt.savefig(os.path.join(\"Feature_Importance_RF-final.png\"), dpi=200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdabff6-63db-48fb-adbb-1e93250a2ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second method, utilizing the code from above to perform similar feature selection using xgboost\n",
    "# Can be tested with and without scaling\n",
    "\n",
    "# Initialize SelectFromModel with XGBRegressor\n",
    "xgb_selector = SelectFromModel(XGBRegressor(random_state=42))\n",
    "\n",
    "# Fit the selector on training data\n",
    "xgb_selector.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Get and print the selected features\n",
    "xgb_features = list(X_train_val.columns[(xgb_selector.get_support())])\n",
    "print(xgb_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcbf7ae-b1a8-438b-83ed-da43ec024927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print plot\n",
    "\n",
    "# Access the XGBoost model to get the feature importances\n",
    "feature_importances = xgb_selector.estimator_.feature_importances_\n",
    "\n",
    "# Get the indices of the features sorted by importance\n",
    "sorted_indices = feature_importances.argsort()[::-1]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Feature Importance using XGBRegressor()', fontsize=18)\n",
    "plt.bar(range(X_train_val.shape[1]), feature_importances[sorted_indices], align='center')\n",
    "plt.xticks(range(X_train_val.shape[1]), X_train_val.columns[sorted_indices], rotation=90)\n",
    "plt.ylabel('Importance')\n",
    "plt.subplots_adjust(bottom=0.3)\n",
    "plt.savefig(os.path.join(\"Feature_Importance_XGB-final.png\"), dpi=200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f03a406-17ae-43c9-b959-ac4778a88f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third method for feature selection, RFE\n",
    "# Using data split without scaling as the random forest is scale-invariant\n",
    "\n",
    "# Initialize the model\n",
    "model = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "\n",
    "# Perform Recursive Feature Elimination (RFE) with a number of selected features\n",
    "rfe = RFE(model, n_features_to_select = 10)\n",
    "rfe.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Get the selected features and safe in a list\n",
    "rfe_features2 = list(X_train_val.columns[rfe.support_])\n",
    "print(rfe_features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90bb0e9-b9ca-4b34-a48d-da01ee53c00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourth method using forward feature selection to determine most relevant features defining n_features_to_select\n",
    "\n",
    "# Initialize model\n",
    "model = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "\n",
    "# Forward Feature Selection\n",
    "sfs_forward = SequentialFeatureSelector(model, n_features_to_select=10, direction='forward')\n",
    "sfs_forward.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Selected features from forward selection and print\n",
    "forward_features2 = list(X_train_val.columns[sfs_forward.get_support()])\n",
    "print(forward_features2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf06379f-07a1-4c90-96da-5f00c061a9e3",
   "metadata": {},
   "source": [
    "## 5. Predictive Modeling\n",
    "\n",
    "The predictive modeling section was divided into three subchapters. The purpose of this thesis is to apply a multitude of methods to answer the research question successfully. This project used predictive modeling to forecast dengue incidence based on environmental variables using supervised learning for regression. The complexity of this multivariate time-series analysis lies in processing data from multiple Southeast Asia locations, each with different time ranges.\n",
    "\n",
    "\n",
    "Scale-variant and invariant models were implemented for the assessment. \n",
    "\n",
    "**Scale-invariant models:**\n",
    "\n",
    "Decision Trees (DT) and Random Forests (RF)\n",
    "- Capable of capturing non-linear relationships and interactions between features.\n",
    "- Prone to overfitting, especially with deep trees; Random Forests are more robust than single-decision trees.\n",
    "- Generally invariant to outliers.\n",
    "\n",
    "XGBoost\n",
    "- Machine learning algorithms under the Gradient Boosting framework. \n",
    "- Less prone to overfitting due to parallel tree boosting than single Decision Trees.\n",
    "- Generally robust, often performing well in regression tasks.\n",
    "\n",
    "AdaBoost\n",
    "- From class sklearn.ensemble\n",
    "- Does not require feature scaling (Here, I tested both alternatives, and the best results were achieved with the unscaled dataset)\n",
    "\n",
    "\n",
    "**Scale-variant models:**\n",
    "\n",
    "SVM (Support Vector Machine)\n",
    "- Effective for capturing non-linear relationships and handling high-dimensional data.\n",
    "- Applicable to regression tasks using non-linear kernels.\n",
    "- Sensitive to outliers.\n",
    "\n",
    "K-Nearest Neighbors (KNN)\n",
    "- Sensitive to outliers due to its focus on the distance between data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2540bfc-a15b-4d68-9ff2-1541ed587f3a",
   "metadata": {},
   "source": [
    "### 5.1 Hyperparameter tuning\n",
    "\n",
    "The fine-tuning process to improve modeling success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a34f1e-3d15-4646-b4da-9862ccc71b49",
   "metadata": {},
   "source": [
    "#### a. Scale-invariant models\n",
    "\n",
    "*DecisionTreeRegressor*:\n",
    "max_depth: Maximum depth of the tree\n",
    "min_samples_split: Minimum number of samples required to split an internal node\n",
    "min_samples_leaf: Minimum number of samples required to be at a leaf node\n",
    "\n",
    "*RandomForestRegressor (in addition to DecisionTreeRegressor parameters)*:\n",
    "n_estimators: Number of trees in the forest\n",
    "max_features: Number of features to consider when looking for the best split\n",
    "\n",
    "*XGBRegressor*:\n",
    "learning_rate: Step size shrinkage used in updates to prevent overfitting\n",
    "max_depth: Maximum depth of a tree\n",
    "n_estimators: Number of boosting rounds\n",
    "\n",
    "*AdaBoostRegressor (similar to XGBRegressor, but with different boosting parameters)*:\n",
    "learning_rate: Shrinks the contribution of each weak learner\n",
    "n_estimators: Number of weak learners to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f95ee5-7931-42a5-9efa-24f2d351aac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performing GRidSearchCV to determine optimal parameters\n",
    "## Here: for AdaBoostRegressor\n",
    "#\n",
    "## Define the grid of hyperparameters to search\n",
    "#param_grid = {\n",
    "#    'n_estimators': [50, 100, 200],  # Number of boosting stages\n",
    "#    'learning_rate': [0.01, 0.02],  # Learning rate shrinks the contribution of each regressor\n",
    "#    'loss': ['linear', 'square', 'exponential']\n",
    "#}\n",
    "#\n",
    "## Set up the GridSearchCV object\n",
    "#grid_search = GridSearchCV(AdaBoostRegressor(), param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "#\n",
    "## Fit the grid search to the training+validation data\n",
    "#grid_search.fit(X_train_val, y_train_val)\n",
    "#\n",
    "## Print the best parameters and the best score\n",
    "#print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "#print(f\"Best cross-validation score: {grid_search.best_score_:.3f}\")\n",
    "#\n",
    "## Evaluate the best model on the test set\n",
    "#best_model = grid_search.best_estimator_\n",
    "#test_score = best_model.score(X_test, y_test)\n",
    "#print(f\"Test set score with best parameters: {test_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8504f-88f6-47cf-917b-e5eaeaf0a3bc",
   "metadata": {},
   "source": [
    "#### b. Scale-variant models\n",
    "\n",
    "*KNeighborsRegressor*:\n",
    "n_neighbors: Number of neighbors to consider.\n",
    "weights: Weight function used in prediction ('uniform' or 'distance').\n",
    "\n",
    "*SVR*:\n",
    "kernel: Specifies the kernel type to be used ('linear', 'poly', 'rbf', 'sigmoid', etc.).\n",
    "C: Regularization parameter.\n",
    "gamma: Kernel coefficient for 'rbf', 'poly', and 'sigmoid'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbf070b-ceb3-4f60-9357-4be3205e340d",
   "metadata": {},
   "source": [
    "### 5.2 Cross-Validation and Model Evaluation on Training Set\n",
    "In the following, the training phase is initialized to evaluate selected models. A time-based cross-validation is applied to handle multiple independent time series for the final independent approach.  The utilized Model Evaluation Metrics are explained as followed: \n",
    "- https://www.geeksforgeeks.org/machine-learning-model-evaluation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90864c2-b1a1-41a5-9ddd-5e6e2c214086",
   "metadata": {},
   "source": [
    "#### Holistic approach with a dependent assumption of the variables\n",
    "\n",
    "Inspiration of dependent/independent approach: \n",
    "- https://cienciadedatos.net/documentos/py44-multi-series-forecasting-skforecast.html#:~:text=In%20independent%20multi%2Dseries%20forecasting,as%20predictors%20of%20other%20series\n",
    "- https://skforecast.org/0.8.0/user_guides/independent-multi-time-series-forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9af2b7c-13ab-4e6b-8a8b-195ab0c30791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holistic approach 1\n",
    "# Performing predictive modeling with a generalized model for all locations\n",
    "# Train Validation Split used with Location Code as differentiator established through LabelEncoder()\n",
    "# Plot actual vs. predicted values of all models filtered by each location\n",
    "\n",
    "def evaluate_models(X_train, X_val, y_train, y_val, models):\n",
    "    # Initialize dictionary to store evaluation metrics\n",
    "    evaluation_metrics = {}\n",
    "    \n",
    "    # Initialize dictionary to store predictions for each model\n",
    "    predictions = {}\n",
    "    \n",
    "    # Iterate through each model\n",
    "    for name, model in models.items():\n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on the val set\n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        # Store predictions in the dictionary\n",
    "        predictions[name] = y_pred\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        mape = mean_absolute_percentage_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        n = X_val.shape[0]\n",
    "        p = X_val.shape[1]\n",
    "        adjusted_r2 = 1 - ((1 - r2) * (n - 1) / (n - p - 1))\n",
    "        \n",
    "        # Update evaluation metrics for the model\n",
    "        evaluation_metrics[name] = {\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE': mape,\n",
    "            'Adjusted R-squared': adjusted_r2,\n",
    "        }\n",
    "        \n",
    "        print(f\"Model: {name}\")\n",
    "        print(\"Mean Absolute Error (MAE):\", mae)\n",
    "        print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    \n",
    "    # Plot actual vs predicted values for each location\n",
    "    for location in X_val['Location Code'].unique():\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Plot actual values\n",
    "        y_val_location = y_val[X_val['Location Code'] == location]\n",
    "        plt.plot(y_val_location.index, y_val_location, label='Actual (y_val)', linewidth=3)\n",
    "        \n",
    "        # Plot predicted values from each model\n",
    "        for name, y_pred in predictions.items():\n",
    "            y_pred_location = y_pred[X_val['Location Code'] == location]\n",
    "            plt.plot(y_val_location.index, y_pred_location, label=f'Predicted ({name})')\n",
    "        \n",
    "        plt.title(f'Comparison of Actual and Predicted Values for {location}')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Target Variable')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    return evaluation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3105b3-7e0d-4c01-b384-e0d610666c3e",
   "metadata": {},
   "source": [
    "#### Individual approach with a independent assumption of the variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cb0260-c440-49ca-8180-958ed4a28cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function returns both location-specific metrics and aggregated metrics\n",
    "# Plot actual vs predicted values for all models in the current location\n",
    "\n",
    "def evaluate_models(X_train_val, y_train_val, models):\n",
    "    # Initialize dictionary to store evaluation metrics for each location and overall\n",
    "    location_metrics = {}\n",
    "    overall_metrics = {name: {'MAE': [], 'RMSE': []} for name in models.keys()}\n",
    "\n",
    "    for location in X_train_val['Location Code'].unique():\n",
    "        location_mask = X_train_val['Location Code'] == location\n",
    "        location_data = X_train_val[location_mask].drop(columns=['Location Code'])\n",
    "        location_target = y_train_val[location_mask]\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        \n",
    "        metrics = {name: {'MAE': [], 'RMSE': []} for name in models.keys()}\n",
    "        predictions_for_plotting = {name: [] for name in models.keys()} \n",
    "\n",
    "        for name, model in models.items():\n",
    "            for train_index, val_index in tscv.split(location_data):\n",
    "                X_train, X_val = location_data.iloc[train_index], location_data.iloc[val_index]\n",
    "                y_train, y_val = location_target.iloc[train_index], location_target.iloc[val_index]\n",
    "\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_val)\n",
    "                y_pred = pd.Series(y_pred, index=y_val.index)\n",
    "                predictions_for_plotting[name].append(y_pred)\n",
    "\n",
    "                # Calculate and store location metrics for each fold\n",
    "                mae = mean_absolute_error(y_val, y_pred)\n",
    "                rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "                #mape = mean_absolute_percentage_error(y_val, y_pred)\n",
    "                #r2 = r2_score(y_val, y_pred)\n",
    "                #n = X_val.shape[0]\n",
    "                #p = X_val.shape[1]\n",
    "                #adjusted_r2 = 1 - ((1 - r2) * (n - 1) / (n - p - 1))\n",
    "\n",
    "                metrics[name]['MAE'].append(mae)\n",
    "                metrics[name]['RMSE'].append(rmse)\n",
    "                #metrics[name]['MAPE'].append(mape)\n",
    "                #metrics[name]['Adjusted R-squared'].append(adjusted_r2)\n",
    "\n",
    "            # Average metrics after all splits for this location\n",
    "            for metric in metrics[name]:\n",
    "                metrics[name][metric] = round(np.mean(metrics[name][metric]), 2)\n",
    "                overall_metrics[name][metric].append(metrics[name][metric])  # Collect metrics across locations\n",
    "\n",
    "            # Print the averaged metrics\n",
    "            print(f\"Averaged metrics for model {name} at location {location}: {metrics[name]}\")\n",
    "\n",
    "        # Plotting after cross-validation for each location and model\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plt.plot(location_target.index, location_target, label='Actual', linewidth=3)\n",
    "        for name in models.keys():\n",
    "            all_preds = pd.concat(predictions_for_plotting[name])\n",
    "            plt.plot(all_preds.index, all_preds, label=f'Predicted by {name}')\n",
    "        plt.title(f'Actual vs Predicted Values for Location {location}')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Incidence Rate')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "            \n",
    "        location_metrics[location] = metrics\n",
    "\n",
    "    # Calculate and print overall average metrics for each model across all locations\n",
    "    for name, metrics in overall_metrics.items():\n",
    "        for metric in metrics:\n",
    "            overall_metrics[name][metric] = round(np.mean(metrics[metric]), 2) if metrics[metric] else np.nan\n",
    "        print(f\"Overall averaged metrics for model {name}: {overall_metrics[name]}\")\n",
    "\n",
    "    return location_metrics, overall_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74676135-8369-4b41-a6ce-c9be77529dc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def visualize_splits(X_train_val, y_train_val, location):\n",
    "    location_mask = X_train_val['Location Code'] == location\n",
    "    location_data = X_train_val[location_mask].drop(columns=['Location Code'])\n",
    "    location_target = y_train_val[location_mask]\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    fig, axes = plt.subplots(nrows=5, ncols=1, figsize=(15, 20))\n",
    "    fig.suptitle(f'Time Series Cross-Validation for Location {location}', fontsize=18)\n",
    "\n",
    "    # Determining the fixed x-axis range based on the whole dataset\n",
    "    x_min, x_max = location_data.index.min(), location_data.index.max()\n",
    "    y_min, y_max = location_target.min(), location_target.max()\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(tscv.split(location_data)):\n",
    "        X_train, X_val = location_data.iloc[train_index], location_data.iloc[val_index]\n",
    "        y_train, y_val = location_target.iloc[train_index], location_target.iloc[val_index]\n",
    "\n",
    "        axes[i].plot(X_train.index, y_train, 'blue', label='Training data')\n",
    "        axes[i].plot(X_val.index, y_val, 'red', label='Validation data')\n",
    "        axes[i].set_title(f'Split {i+1}')\n",
    "        axes[i].set_xlim([x_min, x_max])\n",
    "        axes[i].set_ylim([y_min, y_max])\n",
    "        axes[i].set_ylabel('Incidence Rate')\n",
    "        axes[i].set_xlabel('Date')\n",
    "        axes[i].legend()\n",
    "        \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(f\"{location}_TimeSeriesSplit.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "visualize_splits(X_train_val, y_train_val, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbec221d-347a-46ba-8c93-bbaf1ec22481",
   "metadata": {},
   "source": [
    "#### a. Scale-invariant models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3ac7c9-0a64-4b86-bcfe-fd012576fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the best parameters for the modeling section\n",
    "best_params = {\n",
    "    'Decision Tree':{},\n",
    "    'Random Forest':{'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 10},\n",
    "    'XGBoost':{'colsample_bytree': 0.7, 'learning_rate': 0.014, 'max_depth': 9, 'min_child_weight': 2, 'n_estimators': 141, 'reg_lambda': 3, 'subsample': 0.956},\n",
    "    'AdaBoost':{'learning_rate': 0.01, 'loss': 'linear'}\n",
    "}\n",
    "\n",
    "# Define chosen models for unscaled datasets\n",
    "scale_invariant_models = {\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=42, **best_params['Decision Tree']), # captures non-linear relationships, prone to overfitting\n",
    "    'Random Forest': RandomForestRegressor(random_state=42, **best_params['Random Forest']), #bagging ensemble method\n",
    "    'XGBoost': XGBRegressor(random_state=42, **best_params['XGBoost']), # boosting ensemble method, improves weak learners, handles non-linear relationships, less prone to overfitting\n",
    "    'AdaBoost': AdaBoostRegressor(random_state=42, **best_params['AdaBoost']), # boosting ensemble method, improve weak learners, handle non-linear relationships, less prone to overfitting\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7155d462-dc39-4b48-b91e-53c80a0b1a9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Performing modeling for tree-based, scale-invariant models on all variables\n",
    "\n",
    "evaluation_metrics_original = evaluate_models(X_train_val, y_train_val, scale_invariant_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0419323-d91d-4ebe-8350-dcbfb3ae854c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Performing modeling for tree-based, scale-invariant models using rf feature selection\n",
    "\n",
    "evaluation_metrics_rf = evaluate_models(X_train_val[rf_features], y_train_val, scale_invariant_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bf5ec8-6df2-4ed6-8d8f-2c830b92f102",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Performing modeling for tree-based, scale-invariant models using xgb feature selection\n",
    "\n",
    "evaluation_metrics_xgb = evaluate_models(X_train_val[xgb_features], y_train_val, scale_invariant_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825969ea-24a4-4209-9778-ab46cd7547e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Performing modeling for tree-based, scale-invariant models using rfe feature selection\n",
    "\n",
    "evaluation_metrics_rfe2 = evaluate_models(X_train_val[rfe_features2], y_train_val, scale_invariant_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fa91c6-f001-4689-999d-56aaaf40cde7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Performing modeling for tree-based, scale-invariant models using forward feature selection\n",
    "\n",
    "evaluation_metrics_forward2 = evaluate_models(X_train_val[forward_features2], y_train_val, scale_invariant_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9e99d1-19be-49d8-9e02-0796185c278c",
   "metadata": {},
   "source": [
    "#### b. Scale-variant models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e09e180-55a1-4673-a889-82f008cd5bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the best parameters for the modeling section\n",
    "best_params = {\n",
    "    #'Logistic Regression':,\n",
    "    'KNN':{'metric': 'euclidean', 'n_neighbors': 10, 'weights': 'distance'},\n",
    "    'Support Vector Regression':{'C': 100, 'kernel': 'rbf'}\n",
    "}\n",
    "\n",
    "# Define a dictionary to hold models\n",
    "scale_variant_models = {\n",
    "    #'Logistic Regression': LogisticRegression(**best_params['Logistic Regression']),\n",
    "    'KNN': KNeighborsRegressor(**best_params['KNN']), # makes predictions based on the similarity of data points\n",
    "    'Support Vector Regression': SVR(**best_params['Support Vector Regression']) #captures non-linear relationships and handling high-dimensional data\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb6c482-d28f-4d84-9154-78dbbf999cff",
   "metadata": {},
   "source": [
    "#### MinMaxScaler\n",
    "\n",
    "Perform modeling for scale-variant models, using a dataset normalized with the MinMaxScaler.\n",
    "\n",
    "\n",
    "Source: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb7b2b-ea38-4fef-95bb-948c6f29fadc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Performing modeling for scale-variant models using minmax scaler on the original dataset\n",
    "\n",
    "evaluation_metrics_minmax_original = evaluate_models(X_train_val_scaled_minmax, y_train_val, scale_variant_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c14527e-3668-4a78-b4e7-b2d4b8632243",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Performing modeling for scale-variant models using minmax scaler on the rf dataset\n",
    "\n",
    "evaluation_metrics_minmax_rf = evaluate_models(X_train_val_scaled_minmax[rf_features], y_train_val, scale_variant_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41639bb-c742-41e8-badb-65e7a32f1b53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Performing modeling for scale-variant models using minmax scaler on the xgb dataset\n",
    "\n",
    "evaluation_metrics_minmax_xgb = evaluate_models(X_train_val_scaled_minmax[xgb_features], y_train_val, scale_variant_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577375c6-a739-412f-8a08-26d953356338",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Performing modeling for scale-variant models using minmax scaler on the rfe dataset with 10 features\n",
    "\n",
    "evaluation_metrics_minmax_rfe2 = evaluate_models(X_train_val_scaled_minmax[rfe_features2], y_train_val, scale_variant_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fc702e-da03-4512-a930-5fd478b21298",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Performing modeling for scale-variant models using minmax scaler on the forward selection dataset with 10 features\n",
    "evaluation_metrics_minmax_forward2 = evaluate_models(X_train_val_scaled_minmax[forward_features2], y_train_val, scale_variant_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb3ccb-c181-4914-85f2-5bd3525d8a97",
   "metadata": {},
   "source": [
    "#### RobustScaler\n",
    "Perform modeling for scale-variant models, using a dataset normalized with the RobustScaler.\n",
    "\n",
    "Source: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec74acb-eb7d-4f22-ab03-e1390dade82a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Performing modeling for scale-variant models using robust scaler on the original dataset\n",
    "\n",
    "evaluation_metrics_robust_original = evaluate_models(X_train_val_scaled_robust, y_train_val, scale_variant_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f9d2fa-8f2c-47d6-8807-e7433ce3a754",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Performing modeling for scale-variant models using robust scaler on the rf dataset\n",
    "\n",
    "evaluation_metrics_robust_rf = evaluate_models(X_train_val_scaled_robust[rf_features], y_train_val, scale_variant_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8036ae75-7b14-41dc-b3a3-2cce3b5027dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Performing modeling for scale-variant models using robust scaler on the xgb dataset\n",
    "\n",
    "evaluation_metrics_robust_xgb = evaluate_models(X_train_val_scaled_robust[xgb_features], y_train_val, scale_variant_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bcec38-3ca6-4666-8bd3-6ec4f7b985cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Performing modeling for scale-variant models using robust scaler on the rfe dataset with 10 features\n",
    "\n",
    "evaluation_metrics_robust_rfe2 = evaluate_models(X_train_val_scaled_robust[rfe_features2], y_train_val, scale_variant_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cbe3e9-9530-45c7-98a9-94121258e889",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Performing modeling for scale-variant models using robust scaler on the forward selection dataset with 5 features\n",
    "\n",
    "evaluation_metrics_robust_forward2 = evaluate_models(X_train_val_scaled_robust[forward_features2], y_train_val, scale_variant_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913aee2d-9e44-4de4-ab8f-6b329f56f5c7",
   "metadata": {},
   "source": [
    "### 5.3 Final Training and Prediction on Test Set\n",
    "\n",
    "Predict unseen data to evaluate model effectiveness in forecasting dengue incidence rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e6d7c1-3e66-4fea-b41a-d2586b431410",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function for the test set\n",
    "\n",
    "def train_and_evaluate_on_test_set(X_train_val, y_train_val, X_test, y_test, models):\n",
    "    \n",
    "    test_metrics = {}\n",
    "    test_predictions = {}\n",
    "    \n",
    "    overall_metrics = {name: {'MAE': [], 'RMSE': []} for name in models.keys()}\n",
    "\n",
    "    for location in X_train_val['Location Code'].unique():\n",
    "        \n",
    "        location_mask_train = X_train_val['Location Code'] == location\n",
    "        location_data_train = X_train_val[location_mask_train].drop(columns=['Location Code'])\n",
    "        location_target_train = y_train_val[location_mask_train]\n",
    "\n",
    "        location_mask_test = X_test['Location Code'] == location\n",
    "        location_data_test = X_test[location_mask_test].drop(columns=['Location Code'])\n",
    "        location_target_test = y_test[location_mask_test]\n",
    "\n",
    "        metrics = {name: {'MAE': None, 'RMSE': None} for name in models.keys()}\n",
    "        predictions_for_plotting = {name: [] for name in models.keys()}\n",
    "\n",
    "        for name, model in models.items():\n",
    "            model.fit(location_data_train, location_target_train)\n",
    "            y_pred_test = model.predict(location_data_test)\n",
    "            y_pred_test = pd.Series(y_pred_test, index=location_target_test.index)\n",
    "            predictions_for_plotting[name].append(y_pred_test)\n",
    "            test_predictions.setdefault(name, {})[location] = y_pred_test\n",
    "\n",
    "            mae = round(mean_absolute_error(location_target_test, y_pred_test), 2)\n",
    "            rmse = round(np.sqrt(mean_squared_error(location_target_test, y_pred_test)), 2)\n",
    "\n",
    "            metrics[name]['MAE'] = mae\n",
    "            metrics[name]['RMSE'] = rmse\n",
    "\n",
    "            overall_metrics[name]['MAE'].append(mae)\n",
    "            overall_metrics[name]['RMSE'].append(rmse)\n",
    "\n",
    "            print(f\"Test metrics for model {name} at location {location}: MAE={mae}, RMSE={rmse}\")\n",
    "\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plt.plot(location_target_test.index, location_target_test, label='Actual', linewidth=3)\n",
    "        for name in models.keys():\n",
    "            plt.plot(predictions_for_plotting[name][0].index, predictions_for_plotting[name][0], label=f'Predicted by {name}')\n",
    "        plt.title(f'Actual vs Predicted Values for Location {location}')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Incidence Rate')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        test_metrics[location] = metrics\n",
    "\n",
    "    # Calculate and print overall average metrics for each model across all locations\n",
    "    for name, metrics in overall_metrics.items():\n",
    "        for metric in metrics:\n",
    "            overall_metrics[name][metric] = round(np.mean(metrics[metric]) if metrics[metric] else np.nan, 2)\n",
    "        print(f\"Overall averaged metrics for model {name}: {overall_metrics[name]}\")\n",
    "\n",
    "    return test_metrics, test_predictions, overall_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4919ca-1dc0-4e52-8f10-aeee31c5f137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the best parameters for the modeling section\n",
    "best_params = {\n",
    "    'Support Vector Regression':{'C': 100, 'kernel': 'rbf'},\n",
    "    'AdaBoost':{'learning_rate': 0.01, 'loss': 'linear'}\n",
    "}\n",
    "\n",
    "# Define a dictionary to hold models\n",
    "scale_variant_models = {\n",
    "    'Support Vector Regression': SVR(**best_params['Support Vector Regression']) #captures non-linear relationships and handling high-dimensional data\n",
    "}\n",
    "\n",
    "# Define chosen models for unscaled datasets\n",
    "scale_invariant_models = {\n",
    "    'AdaBoost': AdaBoostRegressor(random_state=42, **best_params['AdaBoost']), # boosting ensemble method, improve weak learners, handle non-linear relationships, less prone to overfitting\n",
    "   }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee8701-0616-4635-8a5d-e7c620d121d9",
   "metadata": {},
   "source": [
    "#### a. Scale-invariant models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2be73f1-fc20-471b-ba4d-c07e24a4c39d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_metrics, test_predictions, overall_metrics = train_and_evaluate_on_test_set(X_train_val[rf_features], y_train_val, X_test[rf_features], y_test, scale_invariant_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bfee2a-db4b-4179-b60b-4687c50d63b4",
   "metadata": {},
   "source": [
    "#### b. Scale-variant models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6302674-fab2-467a-a099-c957991259fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_metrics, test_predictions, overall_metrics = train_and_evaluate_on_test_set(X_train_val_scaled_robust , y_train_val, X_test_scaled_robust, y_test, scale_variant_models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
