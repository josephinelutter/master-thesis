{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "044a7955-4349-48f7-8bec-7b62b8892ad3",
   "metadata": {},
   "source": [
    "## Notebook 4 - All locations\n",
    "### **Predicting Dengue Fever Incidence and Disease Dynamics under Climate Change in Southeast Asia**\n",
    "### Master's thesis by Josephine Lutter, supervised by Professor Roberto Henriques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d20110-480c-4f70-8c7a-a8a8a516eb25",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<ul>\n",
    "  <li><a href=\"#1.-Import\">1. Import</a></li>\n",
    "  <li><a href=\"#2.-Data-Preparation\">2. Data Preparation</a>\n",
    "    <ul>\n",
    "      <li><a href=\"#a.-Feature-Removal\">a. Feature Removal</a></li>\n",
    "      <li><a href=\"#b.-Feature-Creation\">b. Feature Creation</a></li>\n",
    "      <li><a href=\"#c.-Data-Encoding\">c. Data Encoding</a></li>\n",
    "      <li><a href=\"#d.-Data-Partition\">d. Data Partition</a></li>\n",
    "      <li><a href=\"#e.-Data-Normalization\">e. Data Normalization</a></li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li><a href=\"#3.-Predictive-Modeling\">3. Predictive Modeling</a>\n",
    "    <ul>\n",
    "      <li><a href=\"#a.-Recurrent-Neural-Network-(RNN)\">a. Recurrent Neural Network (RNN)</a></li>\n",
    "      <li><a href=\"#b.-Feedforward-Neural-Network-(FNN)\">b. Feedforward Neural Network (FNN)</a></li>\n",
    "      <li><a href=\"#c.-Convolutional-Neural-Network-(CNN)\">c. Convolutional Neural Network (CNN)</a></li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li><a href=\"#3.1-Final-Training-and-Prediction-on-Test-Set\">3.1 Final Training and Prediction on Test Set</a></li>\n",
    "  <li><a href=\"#3.2-Climate-Change-Assessment\">3.2 Climate Change Assessment</a></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ddd05-a48c-4663-8571-486c01cde612",
   "metadata": {},
   "source": [
    "## 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e340407-86b1-4cb2-ac8d-20a7f424fcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scipy\n",
    "#!pip install tensorflow\n",
    "#!pip install keras-self-attention\n",
    "#!pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
    "#!pip install bayesian-optimization\n",
    "#!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52f1df4-ffb9-4444-9320-e16b53f62893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from numpy import concatenate\n",
    "from pandas import read_csv, DataFrame, concat\n",
    "\n",
    "# Imports for Data Exploration\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "# Imports for Data Engineering / Preprocessing\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.feature_selection import RFE, SequentialFeatureSelector, SelectFromModel\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Imports for Traditional Machine Learning Modeling\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "# Imports for Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Input, Conv1D, MaxPooling1D, Flatten, GRU, SimpleRNN\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, Nadam, legacy\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import keras_tuner\n",
    "from keras_tuner import HyperModel\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# Set seed value to ensure constant, deterministic prediction\n",
    "random_seed = 1234\n",
    "os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "# Lastly, setting random seed for modeling\n",
    "import random\n",
    "random.seed(random_seed)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Check tensorflow version\n",
    "#print(tf.__version__)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "# Ensure reproducibility for sklearn as well\n",
    "from sklearn.utils import check_random_state\n",
    "check_random_state(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d2bb6-07ba-48d8-bd90-3f74986d1327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration of Excel files that have been pre-structured using Mircosoft Excel to guarantee consistent structure\n",
    "\n",
    "file = \"/Users/Fine/Documents/Master Business Analytics/Thesis/Research Data/Final Data/Combined//All locations combined.xlsx\"\n",
    "#file = \"/Users/Fine/Documents/Master Business Analytics/Thesis/Research Data/Final Data/Combined//Simulations RCP8.5_mod.xlsx\"\n",
    "df_3 = pd.read_excel(file, sheet_name='With outlier capping')\n",
    "#df_3 = pd.read_excel(file, sheet_name='RCP8.5, Jan, 2030')\n",
    "#df_3 = pd.read_excel(file, sheet_name='RCP8.5, Jul, 2030')\n",
    "#df_3 = pd.read_excel(file, sheet_name='RCP8.5, Jan, 2050')\n",
    "#df_3 = pd.read_excel(file, sheet_name='RCP8.5, Jul, 2050')\n",
    "#df_3 = pd.read_excel(file, sheet_name='RCP8.5, Jan, 2070')\n",
    "#df_3 = pd.read_excel(file, sheet_name='RCP8.5, Jul, 2070')\n",
    "#df_3 = pd.read_excel(file, sheet_name='RCP8.5, Jan, 2100')\n",
    "#df_3 = pd.read_excel(file, sheet_name='RCP8.5, Jul, 2100')\n",
    "\n",
    "# Define the location for plots\n",
    "location = 'Southeast Asia'\n",
    "\n",
    "# Set Date variable as index and to remain cohesive time series, round, and sort index 'Date' of df_3\n",
    "df_3.set_index(\"Date\", inplace=True)\n",
    "df_3 = df_3.round(4).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de447703-6f18-4f0e-8f43-e224c13c9e20",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "The EDA of the monthly combined and aggregated environmental and incidence data was performed in notebook 3. This notebook covers the implementation of the deep learning models. Here, the data preparation differs from the traditional machine learning models, with most steps being performed within the functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3ba074-375e-433b-bbfa-8c6c53b393f8",
   "metadata": {},
   "source": [
    "### a. Feature Removal\n",
    "\n",
    "Removing features that are identified as irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f2ac8e-b520-41cf-aa7e-00082cfefcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the highlighted variables from notebook 3\n",
    "\n",
    "df_3 = df_3.drop(columns=['Min_Daily_Prcp', 'Monthly_Avg_Prcp', 'Min_Average_Temp', 'Max_Average_Temp'])\n",
    "\n",
    "# Define list\n",
    "environmental_variables = [\n",
    "    \"Max_Daily_Prcp\", \"Monthly_Total_Prcp\",\"Monthly_Avg_Temp\", \n",
    "    \"Min_Daily_Temp\", \"Max_Daily_Temp\", \"N_Raining_Days\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496bbb76-39fb-484d-a70b-2f9b602b4567",
   "metadata": {},
   "source": [
    "### b. Feature Creation\n",
    "\n",
    "Having defined relevant environmental variables in the previous sections, the objective is to create lagged variables for optimal time-series forecasting. At first, the lagged intervals will be determined. As discussed in the literature review, the total dengue infection cycle lasts 4-7 weeks. Moreover, as elaborated, environmental factors immensely influence the mosquito habitat and lifecycle. Consequently, lagged variables will be established for each environmental factor at multiple intervals before the current observation, considering that the data is aggregated monthly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f596505-701a-4074-9af6-e22678fef24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of lagged features\n",
    "# Lag intervals from 1 to 6 months\n",
    "# Meaning, missing values will be created in the first five instances as the data is not giving\n",
    "# Rows with missing values will be excluded to avoid subsequent issues\n",
    "\n",
    "# Define interval\n",
    "lagged_intervals = range(1, 7)\n",
    "new_features = []\n",
    "\n",
    "# Create lags\n",
    "for variable in environmental_variables:\n",
    "    for lag in lagged_intervals:\n",
    "        new_feature_name = f\"{variable}_lag_{lag}\"\n",
    "        df_3[new_feature_name] = df_3[variable].shift(lag)\n",
    "        new_features.append(new_feature_name)\n",
    "\n",
    "# Update environmental_variables\n",
    "environmental_variables.extend(new_features)\n",
    "\n",
    "# Drop rows with missing values as they are represented in the lags\n",
    "df_3.dropna(inplace=True)\n",
    "\n",
    "# Visualize the engineered data \n",
    "df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c7fcb-25e4-477a-ae44-36fe07913f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many features are established after feature engineering\n",
    "len(environmental_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53ddce3-cb2c-4103-a26f-618e44635cfd",
   "metadata": {},
   "source": [
    "### c. Data Encoding\n",
    "Encoding of categorical variable \"Name\". Source of inspiration: https://medium.com/aiskunks/categorical-data-encoding-techniques-d6296697a40f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabe7125-1f03-4525-ab43-0950778ae401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the variable Name with LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "df_3['Location Code'] = LabelEncoder().fit_transform(df_3['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9729a49-84dc-4e9c-b2ff-ffe644da15c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print location names and their respective location codes\n",
    "df_3[['Name', 'Location Code']].drop_duplicates().sort_values('Location Code')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb41231-beac-4fe6-a629-cf33e6f86a4b",
   "metadata": {},
   "source": [
    "### d. Data Partition\n",
    "\n",
    "At this stage, a split is performed to separate a test set from the training phase for the later assessment of the model's success on unseen data.\n",
    "\n",
    "Here, only the location-specific individual approach is pursued, not the ineffective evaluated holistic model from Notebook 3. The later applied time-series cross-validation aligns with an expanding window training approach. In addition, a time-based split is performed at 80%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db0da07-5899-447f-aeab-5fdeb968787e",
   "metadata": {},
   "source": [
    "#### Individual approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49814d57-8b55-4e3c-b4ee-88daf5b8dceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform time-based cross-validation\n",
    "\n",
    "# Time-series split to handle multiple independent time series of training and test set\n",
    "# Test set represents the last 12 monthly instances representing a year\n",
    "# This one is for the use of later cross-validation\n",
    "\n",
    "# Specify features and target\n",
    "features = ['Location Code'] + environmental_variables\n",
    "target = 'Incidence Rate'\n",
    "\n",
    "# Divide the DataFrame into features (X) and target (y)\n",
    "X = df_3[features]#.astype('float32')\n",
    "y = df_3[target]#.astype('float32')\n",
    "\n",
    "# Prepare lists to hold split data\n",
    "train_dfs, test_dfs, y_train_dfs, y_test_dfs = [], [], [], []\n",
    "\n",
    "# Perform the split for each unique location\n",
    "for location in X['Location Code'].unique():\n",
    "    # Filter rows for the current location\n",
    "    location_mask = X['Location Code'] == location\n",
    "    X_location = X[location_mask]\n",
    "    y_location = y[location_mask]\n",
    "\n",
    "    # Calculate the train size by excluding the last 12 instances, which represents a year\n",
    "    train_size = len(X_location) - 12\n",
    "\n",
    "    # Split the data into train and test sets for the current location\n",
    "    X_train_val, X_test = X_location.iloc[:train_size], X_location.iloc[train_size:]\n",
    "    y_train_val, y_test = y_location.iloc[:train_size], y_location.iloc[train_size:]\n",
    "\n",
    "    # Append the splits to their respective lists\n",
    "    train_dfs.append(X_train_val)\n",
    "    test_dfs.append(X_test)\n",
    "    y_train_dfs.append(y_train_val)\n",
    "    y_test_dfs.append(y_test)\n",
    "    \n",
    "# Concatenate the splits into training and testing sets\n",
    "X_train_val, X_test = pd.concat(train_dfs).sort_index(), pd.concat(test_dfs).sort_index()\n",
    "y_train_val, y_test = pd.concat(y_train_dfs).sort_index(), pd.concat(y_test_dfs).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c886ac9c-2abf-4c61-80b9-a1734d3d7ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Individual, independent approach\n",
    "\n",
    "# Visualization of the train-validation-test split by location\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot data for each unique location\n",
    "for location in df_3['Location Code'].unique(): \n",
    "    # Plot training data\n",
    "    train_subset = X_train_val[X_train_val['Location Code'] == location]\n",
    "    plt.scatter(train_subset.index, [location] * len(train_subset), color='blue', alpha=0.5)\n",
    "\n",
    "    # Plot testing data\n",
    "    test_subset = X_test[X_test['Location Code'] == location]\n",
    "    plt.scatter(test_subset.index, [location] * len(test_subset), color='red', alpha=0.5)\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Location Code')\n",
    "plt.title('Train-Test Split by Location Code', fontsize=18)\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.grid(True)\n",
    "plt.xticks()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(f\"{location}:Train-Test-Split-for-each-location.png\"), dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7f151d-1228-4b36-ba5e-dd4ac85d9828",
   "metadata": {},
   "source": [
    "### e. Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec368fc-4a0a-4961-9384-2ce451c6b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for feature scaling\n",
    "\n",
    "def feature_scaling(X_train, X_test, numerical_features, scaler):\n",
    "\n",
    "    # Apply scaling\n",
    "    X_train_scaled = scaler.fit_transform(X_train[numerical_features])\n",
    "    X_test_scaled = scaler.transform(X_test[numerical_features])\n",
    "    \n",
    "    # Convert scaled arrays back to DataFrame, ensuring correct column names and index\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=numerical_features, index=X_train.index)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=numerical_features, index=X_test.index)\n",
    "    \n",
    "    # Combine scaled and non-scaled features\n",
    "    X_train_final = pd.concat([X_train.drop(columns=numerical_features), X_train_scaled], axis=1)\n",
    "    X_test_final = pd.concat([X_test.drop(columns=numerical_features), X_test_scaled], axis=1)\n",
    "    \n",
    "    return X_train_final, X_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee051a2-cef1-4644-a274-2fad154359d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for inverse scaling\n",
    "\n",
    "def inverse_feature_scaling(X_train_scaled, X_test_scaled, numerical_features, scaler):\n",
    "\n",
    "    # Inverse scaling\n",
    "    X_train_inverse_scaled = scaler.inverse_transform(X_train_scaled[numerical_features])\n",
    "    X_test_inverse_scaled = scaler.inverse_transform(X_test_scaled[numerical_features])\n",
    "    \n",
    "    # Convert inverse scaled arrays back to DataFrame, ensuring correct column names and index\n",
    "    X_train_inverse_scaled = pd.DataFrame(X_train_inverse_scaled, columns=numerical_features, index=X_train_scaled.index)\n",
    "    X_test_inverse_scaled = pd.DataFrame(X_test_inverse_scaled, columns=numerical_features, index=X_test_scaled.index)\n",
    "    \n",
    "    # Combine inverse scaled and non-scaled features\n",
    "    X_train_final = pd.concat([X_train_scaled.drop(columns=numerical_features), X_train_inverse_scaled], axis=1)\n",
    "    X_test_final = pd.concat([X_test_scaled.drop(columns=numerical_features), X_test_inverse_scaled], axis=1)\n",
    "    \n",
    "    return X_train_final, X_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92854d35-a813-4d9f-abac-4715f25c37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val_scaled_minmax, X_test_scaled_minmax = feature_scaling(X_train_val, X_test, environmental_variables, MinMaxScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79f123e-46df-4808-a085-668399f0e4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_robust, X_val_scaled_robust = feature_scaling(X_train_val, X_test, environmental_variables, RobustScaler())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36eb50d6-7e47-4b99-a902-83f2dea9bba9",
   "metadata": {},
   "source": [
    "## 3. Predictive Modeling\n",
    "In this notebook, predictive modeling is performed using deep learning models. Several requirements were addressed: Scaling and encoding data in float format, creating lagged features and removing non-informative ones, setting a random seed for reproducibility, and appropriately splitting the data to address the time-series problem. Data is sorted by index Date, which is a time-based feature. \n",
    "\n",
    "Various deep learning models with multiple layers suitable for sequential data analysis, such as time series, were applied. Different model configurations were tested. Relatively low layers and early stopping criteria were implemented to avoid overfitting, which would stop the training process if no improvement was observed after 5 consecutive epochs.\n",
    "\n",
    "Research used: \n",
    "\n",
    "- https://towardsdatascience.com/time-series-forecasting-with-deep-learning-and-attention-mechanism-2d001fc871fc\n",
    "- https://towardsdatascience.com/how-to-reshape-data-and-do-regression-for-time-series-using-lstm-133dad96cd00\n",
    "- https://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b\n",
    "- https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/\n",
    "- https://medium.com/intel-tech/how-to-apply-transformers-to-time-series-models-spacetimeformer-e452f2825d2e\n",
    "- https://www.sciencedirect.com/topics/computer-science/deep-learning-model\n",
    "- https://www.ibm.com/topics/deep-learning\n",
    "- https://doi.org/10.1016/j.undsp.2023.05.006\n",
    "- https://doi.org/10.1016/j.chaos.2020.110121\n",
    "- https://medium.com/aimonks/multivariate-timeseries-analysis-using-tensorflow-9554e607077a\n",
    "- https://towardsdatascience.com/multivariate-time-series-forecasting-with-deep-learning-3e7b3e2d2bcf\n",
    "- https://github.com/Alro10/deep-learning-time-series?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7e5325-c85d-4746-b80e-714f9a871ef6",
   "metadata": {},
   "source": [
    "### a. RNN (Recurrent Neural Network)\n",
    "\n",
    "Appllied are RNN and its evolutions developed to overcome the vanishing gradient problem. A helpful source of inspiration for multivariate time series forecasting with LSTM:\n",
    "- https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab95379-c176-4f25-8794-020f49ae3a0f",
   "metadata": {},
   "source": [
    "#### SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179faa98-5921-483f-b354-5f278e87e842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Simple RNN (Recurrent Neural Network)\n",
    "# Location-specific approach, without k-fold cross-validation\n",
    "\n",
    "# Initialize lists to store MAE and RMSE scores for each location\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "\n",
    "# Loop through each location\n",
    "for location in X_train_val_scaled_minmax['Location Code'].unique():\n",
    "    location_mask = X_train_val_scaled_minmax['Location Code'] == location\n",
    "    X_location = X_train_val_scaled_minmax[location_mask].drop(columns=['Location Code']).values.astype('float32')\n",
    "    y_location = y_train_val[location_mask].values.astype('float32')\n",
    "\n",
    "    # Regular train-validation split with respect to time-series\n",
    "    train_size = int(len(X_location) * 0.8)\n",
    "    X_train, X_val = X_location[0:train_size], X_location[train_size:]\n",
    "    y_train, y_val = y_location[0:train_size], y_location[train_size:]\n",
    "    \n",
    "    # Reshape input to be 3D [samples, timesteps, features] for RNN\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_val = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "        \n",
    "    # Define and compile the model\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dense(1)) # Output layer for regression\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "    # Define early stopping criteria\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) \n",
    "    # Based on validation loss, the model will stop training if no improvement after five 5 cohesive epochs\n",
    "    \n",
    "    # Fit neural network\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=72, \n",
    "                        validation_data=(X_val, y_val), \n",
    "                        callbacks=[early_stopping], \n",
    "                        verbose=2, \n",
    "                        shuffle=False)    \n",
    "    \n",
    "    # Prediction and error calculation\n",
    "    y_pred = model.predict(X_val)\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[2]))\n",
    "    X_val = X_val.reshape((X_val.shape[0], X_val.shape[2]))\n",
    "        \n",
    "    # Perform and save metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "\n",
    "    # Plotting results and prediction comparisons\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # Plot for actual vs predicted\n",
    "    plt.subplot(1, 3, 1)  # 3 plots, this is the first\n",
    "    plt.plot(y_val, label='Actual', color='blue')\n",
    "    plt.plot(y_pred, label='Predicted', color='red')\n",
    "    plt.title(f'Actual vs Predicted for Location: {location}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Incidence')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot for training and validation loss\n",
    "    plt.subplot(1, 3, 2)  # This is the second plot\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    # Print error metrics for this location\n",
    "    print(f'MAE: {mae}, RMSE: {rmse}')\n",
    "\n",
    "# Calculate average MAE and RMSE scores\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "\n",
    "# Print average scores\n",
    "print(f'Average MAE: {avg_mae}, Average RMSE: {avg_rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb66ff4f-b2c8-4678-8b8e-0ff9498d8b5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cross-validation for SimpleRNN\n",
    "# Source: https://machinelearningmastery.com/understanding-simple-recurrent-neural-networks-in-keras/\n",
    "\n",
    "# Set random seeds for constant result\n",
    "def set_random_seeds(seed_value=42):\n",
    "    np.random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    \n",
    "set_random_seeds(42)\n",
    "# Initialize lists to store MAE and RMSE scores for each location\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "\n",
    "# Loop through each location\n",
    "for location in X_train_val_scaled_minmax['Location Code'].unique():\n",
    "    location_mask = X_train_val_scaled_minmax['Location Code'] == location\n",
    "    X_location = X_train_val_scaled_minmax[location_mask].drop(columns=['Location Code']).values.astype('float32')\n",
    "    y_location = y_train_val[location_mask].values.astype('float32')\n",
    "\n",
    "    all_actuals = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    # Time-series cross-validation with k-fold=5\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    location_mae_scores = []\n",
    "    location_rmse_scores = []\n",
    "    \n",
    "    for train_index, test_index in tscv.split(X_location):\n",
    "        X_train, X_val = X_location[train_index], X_location[test_index]\n",
    "        y_train, y_val = y_location[train_index], y_location[test_index]\n",
    "        \n",
    "        # Reshape data for RNN (samples, timesteps, features)\n",
    "        X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "        X_val = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "        \n",
    "        # Define and compile the SimpleRNN model\n",
    "        model = Sequential()\n",
    "        model.add(SimpleRNN(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2]))) #SimpleRNN layer \n",
    "        model.add(Dense(1)) #Output layer\n",
    "        model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "        # Define early stopping criteria\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "        # Fit neural network\n",
    "        history = model.fit(X_train, y_train, epochs=50, batch_size=72, \n",
    "                            validation_data=(X_val, y_val), \n",
    "                            callbacks=[early_stopping], \n",
    "                            verbose=2, \n",
    "                            shuffle=False)\n",
    "        \n",
    "        # Prediction and error calculation\n",
    "        y_pred = model.predict(X_val).flatten()\n",
    "        \n",
    "        # Collect predictions and actual values\n",
    "        all_actuals.extend(y_val)\n",
    "        all_predictions.extend(y_pred)\n",
    "    \n",
    "        # Calculate metrics and save them\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        location_mae_scores.append(mae)\n",
    "        location_rmse_scores.append(rmse)\n",
    "    \n",
    "    # Append location-specific scores to the overall scores\n",
    "    mae_scores.extend(location_mae_scores)\n",
    "    rmse_scores.extend(location_rmse_scores)\n",
    "    \n",
    "    # Plotting predictions vs actual values for the entire location\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(all_actuals, label='Actual', color='blue')\n",
    "    plt.plot(all_predictions, label='Predicted', color='red')\n",
    "    plt.title(f'Actual vs Predicted for Location: {location}')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Incidence Rate')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Average MAE for location {location}: {np.mean(location_mae_scores)}, Average RMSE for location {location}: {np.mean(location_rmse_scores)}')\n",
    "\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "print(f'Average MAE of SimpleRNN: {avg_mae}, Average RMSE of SimpleRNN: {avg_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64715ab1-8724-42a2-8b97-e1394dd25ea3",
   "metadata": {},
   "source": [
    "#### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe085b2a-dace-48cb-87af-83f17c14c124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GRU (Gated Recurrent Unit)\n",
    "# Location-specific approach, without k-fold cross-validation\n",
    "# Simpler architecture than LSTM with a single update gate\n",
    "# Deal with the vanishing gradient problem\n",
    "\n",
    "# Initialize lists to store MAE and RMSE scores for each location\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "\n",
    "# Loop through each location\n",
    "for location in X_train_val_scaled_minmax['Location Code'].unique():\n",
    "    location_mask = X_train_val_scaled_minmax['Location Code'] == location\n",
    "    X_location = X_train_val_scaled_minmax[location_mask].drop(columns=['Location Code']).values.astype('float32')\n",
    "    y_location = y_train_val[location_mask].values.astype('float32')\n",
    "\n",
    "    # Regular train-validation split with respect to time-series\n",
    "    train_size = int(len(X_location) * 0.8)\n",
    "    \n",
    "    X_train, X_val = X_location[0:train_size], X_location[train_size:]\n",
    "    y_train, y_val = y_location[0:train_size], y_location[train_size:]\n",
    "    \n",
    "    # Reshape input to be 3D [samples, timesteps, features] for GRU\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_val = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "        \n",
    "    # Define and compile the model with GRU\n",
    "    model = Sequential()\n",
    "    model.add(GRU(50, input_shape=(X_train.shape[1], X_train.shape[2]))) # 1 GRU layer with 50 units \n",
    "    model.add(Dense(1)) # Dense layer\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "    # Define early stopping criteria\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    # Fit neural network\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=72, \n",
    "                        validation_data=(X_val, y_val), \n",
    "                        callbacks=[early_stopping], \n",
    "                        verbose=2,\n",
    "                        shuffle=False)    \n",
    "    \n",
    "    # Prediction and error calculation\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Perform and save metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "\n",
    "    # Creating the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_val, label='Actual', color='blue')\n",
    "    plt.plot(y_pred, label='Predicted', color='red')\n",
    "    plt.title(f'Actual vs Predicted for Location: {location}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Incidence')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'MAE: {mae}, RMSE: {rmse}')\n",
    "\n",
    "# Calculate average MAE and RMSE scores\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "\n",
    "# Print average scores\n",
    "print(f'Average MAE: {avg_mae}, Average RMSE: {avg_rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62b3336-7d00-47a2-a249-97804c6c2653",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GRU with cross-validation\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_random_seeds(seed_value=42):\n",
    "    np.random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    \n",
    "set_random_seeds(42)\n",
    "\n",
    "# Initialize lists to store MAE and RMSE scores for each location\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "\n",
    "# Loop through each location\n",
    "for location in X_train_val_scaled_minmax['Location Code'].unique():\n",
    "    location_mask = X_train_val_scaled_minmax['Location Code'] == location\n",
    "    X_location = X_train_val_scaled_minmax[location_mask].drop(columns=['Location Code']).values.astype('float32')\n",
    "    y_location = y_train_val[location_mask].values.astype('float32')\n",
    "\n",
    "    all_actuals = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    # Time-series cross-validation with k-fold=5\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    location_mae_scores = []\n",
    "    location_rmse_scores = []\n",
    "    \n",
    "    for train_index, test_index in tscv.split(X_location):\n",
    "        X_train, X_val = X_location[train_index], X_location[test_index]\n",
    "        y_train, y_val = y_location[train_index], y_location[test_index]\n",
    "        \n",
    "        # Reshape data for GRU (samples, timesteps, features)\n",
    "        X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "        X_val = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "        \n",
    "        # Define and compile the GRU model\n",
    "        model = Sequential()\n",
    "        model.add(GRU(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2]))) # 1 GRU layer\n",
    "        model.add(Dense(1)) # Output layer\n",
    "        model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "        # Define early stopping criteria\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "        # Fit neural network\n",
    "        history = model.fit(X_train, y_train, epochs=50, batch_size=72, \n",
    "                            validation_data=(X_val, y_val), \n",
    "                            callbacks=[early_stopping], \n",
    "                            verbose=2, \n",
    "                            shuffle=False)\n",
    "        \n",
    "        # Prediction and error calculation\n",
    "        y_pred = model.predict(X_val).flatten()\n",
    "        \n",
    "        # Collect predictions and actual values\n",
    "        all_actuals.extend(y_val)\n",
    "        all_predictions.extend(y_pred)\n",
    "    \n",
    "        # Calculate metrics and save them\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        location_mae_scores.append(mae)\n",
    "        location_rmse_scores.append(rmse)\n",
    "    \n",
    "    # Append location-specific scores to the overall scores\n",
    "    mae_scores.extend(location_mae_scores)\n",
    "    rmse_scores.extend(location_rmse_scores)\n",
    "    \n",
    "    # Plotting predictions vs actual values for the entire location\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(all_actuals, label='Actual', color='blue')\n",
    "    plt.plot(all_predictions, label='Predicted', color='red')\n",
    "    plt.title(f'Actual vs Predicted for Location: {location}')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Incidence Rate')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Average MAE for location {location}: {np.mean(location_mae_scores)}, Average RMSE for location {location}: {np.mean(location_rmse_scores)}')\n",
    "\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "print(f'Average MAE of GRU: {avg_mae}, Average RMSE of GRU: {avg_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc31d1f-7beb-4096-908a-53cb83c5415d",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21624919-525e-4005-890f-60c8b3964617",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LSTM (Long short-term memory)\n",
    "# Location-specific approach, without k-fold cross-validation\n",
    "# Iterative for each location\n",
    "# Without time-series split cross-validation\n",
    "\n",
    "# Initialize lists to store MAE and RMSE scores for each location\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "\n",
    "# Loop through each location\n",
    "for location in X_train_val_scaled_minmax['Location Code'].unique():\n",
    "    location_mask = X_train_val_scaled_minmax['Location Code'] == location\n",
    "    X_location = X_train_val_scaled_minmax[location_mask].drop(columns=['Location Code']).values.astype('float32')\n",
    "    y_location = y_train_val[location_mask].values.astype('float32')\n",
    "\n",
    "    # Regular train-validation split with respect to time-series\n",
    "    train_size = int(len(X_location) * 0.8)\n",
    "    \n",
    "    X_train, X_val = X_location[0:train_size], X_location[train_size:]\n",
    "    y_train, y_val = y_location[0:train_size], y_location[train_size:]\n",
    "    \n",
    "    # Reshape input to be 3D [samples, timesteps, features] for LSTM\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_val = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "    #print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "    \n",
    "    # Define and compile the model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2]))) # Number of LSTM units (neurons) in the layer\n",
    "    model.add(Dense(1)) # Dense layer, that is fully connected layer that outputs one unit\n",
    "    model.compile(loss='mae', optimizer='adam') #MAE as loss and adam optimizer\n",
    "    #Model summary\n",
    "    #model.summary()\n",
    "\n",
    "    # Define early stopping criteria\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    # Fit neural network\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=72, \n",
    "                        validation_data=(X_val, y_val), \n",
    "                        callbacks=[early_stopping], \n",
    "                        verbose=2,\n",
    "                        shuffle=False)    \n",
    "    \n",
    "    # Plot history\n",
    "    #pyplot.plot(history.history['loss'], label='Training Loss')\n",
    "    #pyplot.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    #pyplot.legend()\n",
    "    #pyplot.show()\n",
    "\n",
    "    # Prediction and error calculation\n",
    "    y_pred = model.predict(X_val)\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[2]))\n",
    "    X_val = X_val.reshape((X_val.shape[0], X_val.shape[2]))\n",
    "        \n",
    "    # Perform and save metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "\n",
    "    # Creating the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_val, label='Actual', color='blue')\n",
    "    plt.plot(y_pred, label='Predicted', color='red')\n",
    "    plt.title(f'Actual vs Predicted for Location: {location}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Incidence')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'MAE: {mae}, RMSE: {rmse}')\n",
    "\n",
    "# Calculate average MAE and RMSE scores\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "\n",
    "# Print average scores\n",
    "print(f'Average MAE: {avg_mae}, Average RMSE: {avg_rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d35a68c-0130-41a5-91d0-8ed407a95a6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LSTM (Long short-term memory)\n",
    "# Location-specific approach, with k-fold cross-validation\n",
    "# Iterative for each location with cross-validation\n",
    "# Scaled dataset\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_random_seeds(seed_value=42):\n",
    "    np.random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    \n",
    "set_random_seeds(42)\n",
    "\n",
    "# Initialize lists to store MAE and RMSE scores for each location\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "\n",
    "# Loop through each location\n",
    "for location in X_train_val_scaled_minmax['Location Code'].unique():\n",
    "    location_mask = X_train_val_scaled_minmax['Location Code'] == location\n",
    "    X_location = X_train_val_scaled_minmax[location_mask].drop(columns=['Location Code']).values.astype('float32')\n",
    "    y_location = y_train_val[location_mask].values.astype('float32')\n",
    "\n",
    "    # Time-series cross-validation with k-fold=5\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    for train_index, test_index in tscv.split(X_location):\n",
    "        X_train, X_val = X_location[train_index], X_location[test_index]\n",
    "        y_train, y_val = y_location[train_index], y_location[test_index]\n",
    "\n",
    "        X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "        X_val = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "        #print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "        \n",
    "        # Define and compile the model\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2]))) #LSTM layer\n",
    "        model.add(Dense(1)) #LSTM layer\n",
    "        model.compile(loss='mae', optimizer='adam')\n",
    "        #Model summary\n",
    "        #model.summary()\n",
    "\n",
    "        # Define early stopping criteria\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        # Fit neural network\n",
    "        history = model.fit(X_train, y_train, epochs=50, batch_size=72, \n",
    "                            validation_data=(X_val, y_val), \n",
    "                            callbacks=[early_stopping], \n",
    "                            shuffle=False)    \n",
    "\n",
    "        # Plot history\n",
    "        #pyplot.plot(history.history['loss'], label='Training Loss')\n",
    "        #pyplot.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        #pyplot.legend()\n",
    "        #pyplot.show()\n",
    "        \n",
    "        # Prediction and error calculation\n",
    "        y_pred = model.predict(X_val)\n",
    "        X_train = X_train.reshape((X_train.shape[0], X_train.shape[2]))\n",
    "        X_val = X_val.reshape((X_val.shape[0], X_val.shape[2])) #check\n",
    "\n",
    "        # Perform and save metrics\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        mae_scores.append(mae)\n",
    "        rmse_scores.append(rmse)\n",
    "\n",
    "        # Creating the plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(y_val, label='Actual', color='blue')\n",
    "        plt.plot(y_pred, label='Predicted', color='red')\n",
    "        plt.title(f'Actual vs Predicted for Location: {location}')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Incidence')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "        print(f'MAE: {mae}, RMSE: {rmse}')\n",
    "\n",
    "# Calculate average MAE and RMSE scores\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "\n",
    "# Print average scores\n",
    "print(f'Average MAE: {avg_mae}, Average RMSE: {avg_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a69644-9492-4dd3-8d34-d26ebed7bb08",
   "metadata": {},
   "source": [
    "### b. Feedforward Neural Network (FNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46308375-9eb0-4c02-8867-0a170349ff3b",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282f8610-7fe7-41a8-9886-99a94d79fd07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Multilayer Perceptron (MLP)\n",
    "# Location-specific approach, without k-fold cross-validation\n",
    "# FNN with two layers, information in one direction\n",
    "\n",
    "# Initialize lists to store MAE and RMSE scores for each location\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "\n",
    "# Loop through each location\n",
    "for location in X_train_val_scaled_minmax['Location Code'].unique():\n",
    "    location_mask = X_train_val_scaled_minmax['Location Code'] == location\n",
    "    X_location = X_train_val_scaled_minmax[location_mask].drop(columns=['Location Code']).values.astype('float32')\n",
    "    y_location = y_train_val[location_mask].values.astype('float32')\n",
    "\n",
    "    # Regular train-validation split with respect to time-series\n",
    "    train_size = int(len(X_location) * 0.8)\n",
    "    X_train, X_val = X_location[0:train_size], X_location[train_size:]\n",
    "    y_train, y_val = y_location[0:train_size], y_location[train_size:]\n",
    "    \n",
    "    # Define and compile the MLP model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=X_train.shape[1])) # Input layer explicitly defined here, first hidden layer\n",
    "    model.add(Dense(32, activation='relu'))  # Second hidden layer\n",
    "    model.add(Dense(1))  # Output layer\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "    # Define early stopping criteria\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    # Fit neural network\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=72, \n",
    "                        validation_data=(X_val, y_val), \n",
    "                        callbacks=[early_stopping], \n",
    "                        verbose=2)\n",
    "\n",
    "    # Prediction and error calculation\n",
    "    y_pred = model.predict(X_val)\n",
    "        \n",
    "    # Perform and save metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "\n",
    "    # Creating the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_val, label='Actual', color='blue')\n",
    "    plt.plot(y_pred, label='Predicted', color='red')\n",
    "    plt.title(f'Actual vs Predicted for Location: {location}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Incidence')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'MAE: {mae}, RMSE: {rmse}')\n",
    "\n",
    "# Calculate average MAE and RMSE scores\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "\n",
    "# Print average scores\n",
    "print(f'Average MAE: {avg_mae}, Average RMSE: {avg_rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da14fee0-7e44-4855-9e5b-ca904d2ec69f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Multilayer Perceptron (MLP)\n",
    "# Location-specific approach, with k-fold cross-validation\n",
    "# Plotting for all splits on each location\n",
    "# FNN with two layers, information in one direction\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_random_seeds(seed_value=42):\n",
    "    np.random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    \n",
    "# Call the function to set seeds\n",
    "set_random_seeds()\n",
    "\n",
    "# Initialize lists to store MAE and RMSE scores for each location\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "\n",
    "# Loop through each location\n",
    "for location in X_train_val_scaled_minmax['Location Code'].unique():\n",
    "    location_mask = X_train_val_scaled_minmax['Location Code'] == location\n",
    "    X_location = X_train_val_scaled_minmax[location_mask].drop(columns=['Location Code']).values.astype('float32')\n",
    "    y_location = y_train_val[location_mask].values.astype('float32')\n",
    "\n",
    "    # Initialize lists to store predictions and actual values for plotting\n",
    "    all_actuals = []\n",
    "    all_predictions = []\n",
    "\n",
    "    # Time-series cross-validation with k-fold=5\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    for train_index, test_index in tscv.split(X_location):\n",
    "        X_train, X_val = X_location[train_index], X_location[test_index]\n",
    "        y_train, y_val = y_location[train_index], y_location[test_index]\n",
    "        \n",
    "        # Define and compile the MLP model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, activation='relu', input_dim=X_train.shape[1])) #First hidden layers\n",
    "        model.add(Dense(32, activation='relu'))  # Additional hidden layer\n",
    "        model.add(Dense(1))  # Output layer\n",
    "        model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "        # Define early stopping criteria\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "        # Fit neural network\n",
    "        history = model.fit(X_train, y_train, epochs=50, batch_size=72, \n",
    "                            validation_data=(X_val, y_val), \n",
    "                            callbacks=[early_stopping], \n",
    "                            verbose=2)\n",
    "        \n",
    "        # Prediction and error calculation\n",
    "        y_pred = model.predict(X_val).flatten()\n",
    "\n",
    "        # Accumulate predictions and actuals\n",
    "        all_predictions.extend(y_pred)\n",
    "        all_actuals.extend(y_val)\n",
    "\n",
    "        # Perform and save metrics\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        mae_scores.append(mae)\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    # Plot all accumulated predictions and actual values after cross-validation\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(all_actuals, label='Actual', color='blue')\n",
    "    plt.plot(all_predictions, label='Predicted', color='red')\n",
    "    plt.title(f'Actual vs Predicted for Location: {location}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Incidence Rate')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'Average MAE for location {location}: {np.mean(mae_scores)}, Average RMSE for location {location}: {np.mean(rmse_scores)}')\n",
    "\n",
    "# Calculate average MAE and RMSE scores\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "\n",
    "# Print average scores\n",
    "print(f'Average MAE of MLP: {avg_mae}, Average RMSE of MLP: {avg_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f85888-15c8-4da6-94d1-b8cee6fd1dd0",
   "metadata": {},
   "source": [
    "### c. Convolutional Neural Network (CNN)\n",
    "\n",
    "Sources: \n",
    "- https://machinelearningmastery.com/how-to-develop-convolutional-neural-networks-for-multi-step-time-series-forecasting/\n",
    "- https://thejaskiran99.medium.com/unlocking-the-potential-of-convolutional-neural-networks-cnns-in-time-series-forecasting-b2fac329e184"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08294eec-fef9-4133-9d30-e9f962a44d55",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3d853a-5680-4009-b1dd-db404bc4e36b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convolutional Neural Networks (CNNs)\n",
    "# Location-specific approach, without k-fold cross-validation\n",
    "# 1 D CNN, suitable for time-series\n",
    "# Kernel goes in one direction only\n",
    "# Source: https://www.tensorflow.org/tutorials/images/cnn\n",
    "\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "\n",
    "# Loop through each location\n",
    "for location in X_train_val_scaled_minmax['Location Code'].unique():\n",
    "    location_mask = X_train_val_scaled_minmax['Location Code'] == location\n",
    "    X_location = X_train_val_scaled_minmax[location_mask].drop(columns=['Location Code']).values.astype('float32')\n",
    "    y_location = y_train_val[location_mask].values.astype('float32')\n",
    "\n",
    "    # Regular train-validation split with respect to time-series\n",
    "    train_size = int(len(X_location) * 0.8)\n",
    "    X_train, X_val = X_location[0:train_size], X_location[train_size:]\n",
    "    y_train, y_val = y_location[0:train_size], y_location[train_size:]\n",
    "    \n",
    "    # Reshape input for CNN\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "    #print(X_train.shape, X_val.shape)\n",
    "    \n",
    "    # Define the CNN model\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)), \n",
    "        # Convolutional 1D layer, larger filter means bigger learning capacity, 3 time steps at a time\n",
    "        MaxPooling1D(pool_size=2), #Pooling layer\n",
    "        Flatten(),\n",
    "        Dense(50, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    #Model summary\n",
    "    #model.summary()\n",
    "\n",
    "    # Early stopping criteria\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    # Fit CNN\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=72, \n",
    "                        validation_data=(X_val, y_val), \n",
    "                        callbacks=[early_stopping], \n",
    "                        verbose=2,\n",
    "                        shuffle=False)\n",
    "    \n",
    "    # Plot history\n",
    "    #pyplot.plot(history.history['loss'], label='Training Loss')\n",
    "    #pyplot.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    #pyplot.legend()\n",
    "    #pyplot.show()\n",
    "\n",
    "    # Prediction and error calculation\n",
    "    y_pred = model.predict(X_val)\n",
    "    #X_train = X_train.reshape((X_train.shape[0], X_train.shape[2]))\n",
    "    #X_val = X_val.reshape((X_val.shape[0], X_val.shape[2]))\n",
    "    \n",
    "    # Calculate metrics and save them\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    \n",
    "    # Plotting predictions vs actual values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_val, label='Actual', color='blue')\n",
    "    plt.plot(y_pred, label='Predicted', color='red')\n",
    "    plt.title(f'Actual vs Predicted for Location: {location}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Incidence')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f'MAE: {mae}, RMSE: {rmse}')\n",
    "\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "print(f'Average MAE: {avg_mae}, Average RMSE: {avg_rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd1380a-30d9-4e73-b90b-dbd117410660",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CNN with cross-validation\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_random_seeds(seed_value=42):\n",
    "    np.random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "\n",
    "# Call the function to set seeds\n",
    "set_random_seeds()\n",
    "\n",
    "# Initialize lists to store MAE and RMSE scores for each location\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "\n",
    "# Loop through each location\n",
    "for location in X_train_val_scaled_minmax['Location Code'].unique():\n",
    "    location_mask = X_train_val_scaled_minmax['Location Code'] == location\n",
    "    X_location = X_train_val_scaled_minmax[location_mask].drop(columns=['Location Code']).values.astype('float32')\n",
    "    y_location = y_train_val[location_mask].values.astype('float32')\n",
    "\n",
    "    # Time-series cross-validation with k-fold=5\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    all_actuals = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    location_mae_scores = []\n",
    "    location_rmse_scores = []\n",
    "    \n",
    "    for train_index, test_index in tscv.split(X_location):\n",
    "        X_train, X_val = X_location[train_index], X_location[test_index]\n",
    "        y_train, y_val = y_location[train_index], y_location[test_index]\n",
    "    \n",
    "        # Reshape input for CNN\n",
    "        X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "        X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "    \n",
    "        # Define the CNN model\n",
    "        model = Sequential([ \n",
    "            Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "            MaxPooling1D(pool_size=2),\n",
    "            Flatten(),\n",
    "            Dense(50, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "        # Early stopping criteria\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        \n",
    "        # Fit CNN\n",
    "        history = model.fit(X_train, y_train, epochs=50, batch_size=72, \n",
    "                            validation_data=(X_val, y_val), \n",
    "                            callbacks=[early_stopping], \n",
    "                            verbose=2,\n",
    "                            shuffle=False)\n",
    "\n",
    "        # Prediction and error calculation\n",
    "        y_pred = model.predict(X_val).flatten()\n",
    "        \n",
    "        # Collect predictions and actual values\n",
    "        all_actuals.extend(y_val)\n",
    "        all_predictions.extend(y_pred)\n",
    "    \n",
    "        # Calculate metrics and save them\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        location_mae_scores.append(mae)\n",
    "        location_rmse_scores.append(rmse)\n",
    "    \n",
    "    # Append location-specific scores to the overall scores\n",
    "    mae_scores.extend(location_mae_scores)\n",
    "    rmse_scores.extend(location_rmse_scores)\n",
    "    \n",
    "    # Plotting predictions vs actual values for the entire location\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(all_actuals, label='Actual', color='blue')\n",
    "    plt.plot(all_predictions, label='Predicted', color='red')\n",
    "    plt.title(f'Actual vs Predicted for Location: {location}')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Incidence Rate')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Average MAE for location {location}: {np.mean(location_mae_scores)}, Average RMSE for location {location}: {np.mean(location_rmse_scores)}')\n",
    "\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "print(f'Average MAE of CNN: {avg_mae}, Average RMSE of CNN: {avg_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97be8406-bd86-4563-883c-e710264b95bb",
   "metadata": {},
   "source": [
    "## 3.1 Final Training and Prediction on Test Set\n",
    "Prediction on test set to evaluate how model generalized on unseen data and forecast annual dengue incidence rate under climate change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8be55c-b536-480b-bd70-96f7adf04620",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MLP on the test set\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_random_seeds(seed_value=42):\n",
    "    np.random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "\n",
    "# Call the function to set seeds\n",
    "set_random_seeds()\n",
    "\n",
    "# Function to train and evaluate the MLP model on the test set\n",
    "def train_and_evaluate_mlp_on_test_set(X_train_val, y_train_val, X_test, y_test):\n",
    "    # Initialize lists to store MAE and RMSE scores for each location\n",
    "    mae_scores = []\n",
    "    rmse_scores = []\n",
    "\n",
    "    # Loop through each location\n",
    "    for location in X_train_val['Location Code'].unique():\n",
    "        \n",
    "        location_mask_train = X_train_val['Location Code'] == location\n",
    "        X_location_train = X_train_val[location_mask_train].drop(columns=['Location Code']).values.astype('float32')\n",
    "        y_location_train = y_train_val[location_mask_train].values.astype('float32')\n",
    "\n",
    "        location_mask_test = X_test['Location Code'] == location\n",
    "        X_location_test = X_test[location_mask_test].drop(columns=['Location Code']).values.astype('float32')\n",
    "        y_location_test = y_test[location_mask_test].values.astype('float32')\n",
    "\n",
    "        # Define and compile the MLP model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, activation='relu', input_shape=(X_location_train.shape[1],)))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "        # Define early stopping criteria\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "        # Fit neural network\n",
    "        history = model.fit(X_location_train, y_location_train, epochs=50, batch_size=72, \n",
    "                            validation_data=(X_location_test, y_location_test), \n",
    "                            callbacks=[early_stopping], \n",
    "                            verbose=2, \n",
    "                            shuffle=False)\n",
    "\n",
    "        # Prediction and error calculation\n",
    "        y_pred = model.predict(X_location_test).flatten()\n",
    "    \n",
    "        # Calculate metrics and save them\n",
    "        rmse = np.sqrt(mean_squared_error(y_location_test, y_pred))\n",
    "        mae = mean_absolute_error(y_location_test, y_pred)\n",
    "        mae_scores.append(mae)\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "        # Plotting predictions vs actual values for the entire location\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(y_location_test, label='Actual', color='blue')\n",
    "        plt.plot(y_pred, label='Predicted', color='red')\n",
    "        plt.title(f'Actual vs Predicted for Location: {location}')\n",
    "        plt.xlabel('Time Step')\n",
    "        plt.ylabel('Incidence Rate')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        print(f'MAE for location {location}: {mae}, RMSE for location {location}: {rmse}')\n",
    "\n",
    "    avg_mae = np.mean(mae_scores)\n",
    "    avg_rmse = np.mean(rmse_scores)\n",
    "    print(f'MAE on Test Set: {avg_mae}, RMSE on Test Set: {avg_rmse}')\n",
    "\n",
    "# Call the function\n",
    "train_and_evaluate_mlp_on_test_set(X_train_val_scaled_minmax, y_train_val, X_test_scaled_minmax, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8a9d56-3446-4921-858e-4d6a566f7737",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Final assessment on the test set using CNN\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_random_seeds(seed_value=42):\n",
    "    np.random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "\n",
    "# Call the function to set seeds\n",
    "set_random_seeds()\n",
    "\n",
    "def train_and_evaluate_on_test_set(X_train_val, y_train_val, X_test, y_test):\n",
    "    # Initialize lists to store MAE and RMSE scores for each location\n",
    "    mae_scores = []\n",
    "    rmse_scores = []\n",
    "\n",
    "    # Loop through each location\n",
    "    for location in X_train_val['Location Code'].unique():\n",
    "        \n",
    "        location_mask_train = X_train_val['Location Code'] == location\n",
    "        X_location_train = X_train_val[location_mask_train].drop(columns=['Location Code']).values.astype('float32')\n",
    "        y_location_train = y_train_val[location_mask_train].values.astype('float32')\n",
    "\n",
    "        location_mask_test = X_test['Location Code'] == location\n",
    "        X_location_test = X_test[location_mask_test].drop(columns=['Location Code']).values.astype('float32')\n",
    "        y_location_test = y_test[location_mask_test].values.astype('float32')\n",
    "\n",
    "        # Reshape input for CNN\n",
    "        X_location_train = X_location_train.reshape((X_location_train.shape[0], X_location_train.shape[1], 1))\n",
    "        X_location_test = X_location_test.reshape((X_location_test.shape[0], X_location_test.shape[1], 1))\n",
    "\n",
    "        # Define the CNN model\n",
    "        model = Sequential([ \n",
    "            Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_location_train.shape[1], 1)),\n",
    "            MaxPooling1D(pool_size=2),\n",
    "            Flatten(),\n",
    "            Dense(50, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "        # Early stopping criteria\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        \n",
    "        # Fit CNN\n",
    "        history = model.fit(X_location_train, y_location_train, epochs=50, batch_size=72, \n",
    "                            validation_data=(X_location_test, y_location_test), \n",
    "                            callbacks=[early_stopping], \n",
    "                            verbose=2,\n",
    "                            shuffle=False)\n",
    "\n",
    "        # Prediction and error calculation\n",
    "        y_pred = model.predict(X_location_test).flatten()\n",
    "    \n",
    "        # Calculate metrics and save them\n",
    "        rmse = np.sqrt(mean_squared_error(y_location_test, y_pred))\n",
    "        mae = mean_absolute_error(y_location_test, y_pred)\n",
    "        mae_scores.append(mae)\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "        # Plotting predictions vs actual values for the entire location\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(y_location_test, label='Actual', color='blue')\n",
    "        plt.plot(y_pred, label='Predicted', color='red')\n",
    "        plt.title(f'Actual vs Predicted for Location: {location}')\n",
    "        plt.xlabel('Time Step')\n",
    "        plt.ylabel('Incidence Rate')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        print(f'MAE for location {location}: {mae}, RMSE for location {location}: {rmse}')\n",
    "\n",
    "    avg_mae = np.mean(mae_scores)\n",
    "    avg_rmse = np.mean(rmse_scores)\n",
    "    print(f'MAE on Test Set: {avg_mae}, RMSE on Test Set: {avg_rmse}')\n",
    "\n",
    "# Call the function\n",
    "train_and_evaluate_on_test_set(X_train_val_scaled_minmax, y_train_val, X_test_scaled_minmax, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ad8ae1f-654d-4505-ae9a-fec171b8f31a",
   "metadata": {},
   "source": [
    "## 3.2 Climate Change Assessment\n",
    "Location Code 5 (Kuching, Malaysia) was chosen for the deployment with the lowest predictive error across all locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676d5252-1314-4a15-98e0-87fb06c5b878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final assessment on the test set for location 5\n",
    "# This function needs to be run with the different datasets to assess the change in annual incidence rate\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_random_seeds(seed_value=42):\n",
    "    np.random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    \n",
    "# Call the function to set seeds\n",
    "set_random_seeds()\n",
    "\n",
    "def train_and_evaluate_on_test_set(X_train_val, y_train_val, X_test, y_test, location_code):\n",
    "    # Filter data for the specified location\n",
    "    location_mask_train = X_train_val['Location Code'] == location_code\n",
    "    X_location_train = X_train_val[location_mask_train].drop(columns=['Location Code']).values.astype('float32')\n",
    "    y_location_train = y_train_val[location_mask_train].values.astype('float32')\n",
    "\n",
    "    location_mask_test = X_test['Location Code'] == location_code\n",
    "    X_location_test = X_test[location_mask_test].drop(columns=['Location Code']).values.astype('float32')\n",
    "    y_location_test = y_test[location_mask_test].values.astype('float32')\n",
    "\n",
    "    # Reshape input for CNN\n",
    "    X_location_train = X_location_train.reshape((X_location_train.shape[0], X_location_train.shape[1], 1))\n",
    "    X_location_test = X_location_test.reshape((X_location_test.shape[0], X_location_test.shape[1], 1))\n",
    "\n",
    "    # Define the CNN model\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_location_train.shape[1], 1)),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(50, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "    # Early stopping criteria\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    # Fit CNN\n",
    "    history = model.fit(X_location_train, y_location_train, epochs=50, batch_size=72, \n",
    "                        validation_data=(X_location_test, y_location_test), \n",
    "                        callbacks=[early_stopping], \n",
    "                        verbose=2,\n",
    "                        shuffle=False)\n",
    "\n",
    "    # Prediction and error calculation\n",
    "    y_pred = model.predict(X_location_test).flatten()\n",
    "    \n",
    "    # Calculate metrics and print\n",
    "    rmse = np.sqrt(mean_squared_error(y_location_test, y_pred))\n",
    "    mae = mean_absolute_error(y_location_test, y_pred)\n",
    "    \n",
    "    # Plotting predictions vs actual values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_location_test, label='Actual', color='blue')\n",
    "    plt.plot(y_pred, label='Predicted', color='red')\n",
    "    plt.title(f'Actual vs Predicted for Location: {location_code}')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Incidence Rate')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f'MAE for location {location_code}: {mae}, RMSE for location {location_code}: {rmse}')\n",
    "    print(f\"Predicted outcome for Location {location_code}: {y_pred[0]:.2f}\")\n",
    "\n",
    "# Call the function\n",
    "train_and_evaluate_on_test_set(X_train_val_scaled_minmax, y_train_val, X_test_scaled_minmax, y_test, location_code=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642c02eb-a0d4-4391-8401-68bcc928a5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scores of the simulated datasets have been saved to visualize the predicted change in the annual incidence rate for both scenarios\n",
    "\n",
    "# Updated data including both scenarios\n",
    "data_rcp4_5 = {\n",
    "    \"Change of mean surface temperature in (C)\": [-0.37, 0.22, 0.60, 0.77, 0.37, 0.52, 0.82, 0.93],\n",
    "    \"Change in monthly mean total precipitation\": [67.72, -10.12, 1.29, 69.40, 143.05, 19.69, 123.08, -9.76],\n",
    "    \"Change in yearly incidence rate for Kuching, MY (%)\": [-2.35, -1.88, -0.94, -0.47, -0.47, -0.94, 0.47, -0.47]\n",
    "}\n",
    "\n",
    "data_rcp8_5 = {\n",
    "    \"Change of mean surface temperature in (C)\": [-0.34, 0.76, -0.20, 0.99, 1.36, 1.87, 2.11, 2.50],\n",
    "    \"Change of monthly mean total precipitation\": [76.69, 10.09, 84.82, -3.09, -76.87, -30.61, 25.80, -156.44],\n",
    "    \"Change in yearly incidence rate for Kuching, MY (%)\": [-1.88, -0.94, -1.88, 1.88, 1.41, 2.35, 2.82, 2.35]\n",
    "}\n",
    "\n",
    "df_rcp4_5 = pd.DataFrame(data_rcp4_5)\n",
    "df_rcp8_5 = pd.DataFrame(data_rcp8_5)\n",
    "\n",
    "# Create the first scatter plot for Temperature change vs Incidence rate change\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_rcp4_5[\"Change of mean surface temperature in (C)\"], df_rcp4_5[\"Change in yearly incidence rate for Kuching, MY (%)\"], color='b', label='RCP4.5')\n",
    "plt.scatter(df_rcp8_5[\"Change of mean surface temperature in (C)\"], df_rcp8_5[\"Change in yearly incidence rate for Kuching, MY (%)\"], color='r', label='RCP8.5')\n",
    "plt.title('Changes in Mean Temperature and Annual Incidence Rate', fontsize=18)\n",
    "plt.xlabel('Change of mean temperature in (C)')\n",
    "plt.ylabel('Change in annual incidence rate for Kuching, MY (%)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the first figure\n",
    "plt.savefig('temperature_change_incidence_rate_scatter.png')\n",
    "\n",
    "# Show the first plot\n",
    "plt.show()\n",
    "\n",
    "# Create the second scatter plot for Precipitation change vs Incidence rate change\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_rcp4_5[\"Change in monthly mean total precipitation\"], df_rcp4_5[\"Change in yearly incidence rate for Kuching, MY (%)\"], color='b', label='RCP4.5')\n",
    "plt.scatter(df_rcp8_5[\"Change of monthly mean total precipitation\"], df_rcp8_5[\"Change in yearly incidence rate for Kuching, MY (%)\"], color='r', label='RCP8.5')\n",
    "plt.title('Changes in Total Precipitation and Annual Incidence Rate', fontsize=18)\n",
    "plt.xlabel('Change in total precipitation (mm)')\n",
    "plt.ylabel('Change in annual incidence rate for Kuching, MY (%)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the second figure\n",
    "plt.savefig('precipitation_change_incidence_rate_scatter.png')\n",
    "\n",
    "# Show the second plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a1831e-bf5d-4f97-b81b-fe1268cca540",
   "metadata": {},
   "outputs": [],
   "source": [
    "## While changes in both total precipitation and mean temperature impact the diseases response, their effects might be interrelated, \n",
    "## with mean temperature appearing to dominate. \n",
    "## Therefore, only simulated changes in total precipitation were applied to assess its influence in annual incidence rate. \n",
    "## At constant mean temperature, total precipitation shows a positive linear trend with changes in yearly incidence rate,\n",
    "## supporting the assumption of an interfering effect between these independent variables. However, given the variability of predicted values,\n",
    "## the informational significance of the simulated changes in total precipitation alone is dubious. \n",
    "\n",
    "## Data for RCP4.5 and RCP8.5\n",
    "#data = {\n",
    "#    \"Scenario\": [\"4.5\"] * 8 + [\"8.5\"] * 8,\n",
    "#    \"Year\": [2030, 2030, 2050, 2050, 2070, 2070, 2100, 2100, 2030, 2030, 2050, 2050, 2070, 2070, 2100, 2100],\n",
    "#    \"Month\": [\"January\", \"July\", \"January\", \"July\", \"January\", \"July\", \"January\", \"July\", \"January\", \"July\", \"January\", \"July\", \"January\", \"July\", \"January\", \"July\"],\n",
    "#    \"Change in total precipitation\": [67.72, -10.12, 1.29, 69.40, 143.05, 19.69, 123.08, -9.76, 76.69, 10.09, 84.82, -3.09, -76.87, -30.61, 25.80, -156.44],\n",
    "#    \"Percentage Change\": [5.94, 5.48, 5.48, 5.94, 6.85, 5.48, 6.39, 5.48, 5.94, 5.48, 5.94, 5.48, 4.57, 5.02, 5.48, 4.11]\n",
    "#}\n",
    "#\n",
    "#df = pd.DataFrame(data)\n",
    "#\n",
    "## Create the scatter plot\n",
    "#plt.figure(figsize=(10, 6))\n",
    "#colors = {'4.5': 'blue', '8.5': 'red'}\n",
    "#for scenario in ['4.5', '8.5']:\n",
    "#    subset = df[df[\"Scenario\"] == scenario]\n",
    "#    plt.scatter(subset[\"Change in total precipitation\"], subset[\"Percentage Change\"], c=colors[scenario], label=f'RCP{scenario}')\n",
    "#\n",
    "#plt.title('Only Precipitation: Changes in Total Precipitation and Annual Incidence Rate', fontsize=18)\n",
    "#plt.xlabel('Change in total precipitation (mm)')\n",
    "#plt.ylabel('Change in annual incidence rate for Kuching, MY (%)')\n",
    "#plt.grid(True)\n",
    "#plt.legend()\n",
    "#plt.tight_layout()\n",
    "#plt.savefig('change_in_precipitation_vs_percentage_change.png')\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
